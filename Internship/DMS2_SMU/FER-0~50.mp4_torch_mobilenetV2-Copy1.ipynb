{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea6d909",
   "metadata": {},
   "source": [
    "# 2023-01-19 13:30\n",
    "민규사원님이 주신 mobilenet v2 model 기반으로 다시 돌려보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94f7d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sldev1/anaconda3/envs/dms2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import cv2\n",
    "from mobilenetv2 import mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49658256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ccb228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_model = mobilenet_v2(in_channels=3, num_classes=8) \n",
    "model_path = './mobilenet_v2_affectnet_2023_01_12_07.37'\n",
    "\n",
    "state = torch.load(model_path, map_location='cpu')\n",
    "emo_model.load_state_dict(state['net'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c678be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "(1, 3, 256, 256)\n",
      "[[[[ 44  28  15 ... 255 250 255]\n",
      "   [255 250 255 ... 199 247 225]\n",
      "   [200 246 225 ... 239 217 184]\n",
      "   ...\n",
      "   [250 237 254 ... 194 236 218]\n",
      "   [195 235 217 ... 231 211 180]\n",
      "   [ 13   5   0 ... 255 254 255]]\n",
      "\n",
      "  [[255 254 255 ... 199 243 226]\n",
      "   [201 243 226 ... 231 211 180]\n",
      "   [ 13   5   0 ... 255 254 254]\n",
      "   ...\n",
      "   [126 153 137 ... 196 180 155]\n",
      "   [  0   0   0 ... 190 165 211]\n",
      "   [190 164 204 ...  41 192 173]]\n",
      "\n",
      "  [[141 176 159 ... 195 180 155]\n",
      "   [  1   0   0 ... 190 165 211]\n",
      "   [190 165 208 ...  70 190 175]\n",
      "   ...\n",
      "   [  1   0   0 ...  21  18  40]\n",
      "   [ 39  36  69 ...  17  31  28]\n",
      "   [ 21  36  34 ...  39  31  26]]]]\n",
      "[[[[255. 250. 255. ...  15.  28.  44.]\n",
      "   [225. 247. 199. ... 255. 250. 255.]\n",
      "   [184. 217. 239. ... 225. 246. 200.]\n",
      "   ...\n",
      "   [218. 236. 194. ... 254. 237. 250.]\n",
      "   [180. 211. 231. ... 217. 235. 195.]\n",
      "   [255. 254. 255. ...   0.   5.  13.]]\n",
      "\n",
      "  [[226. 243. 199. ... 255. 254. 255.]\n",
      "   [180. 211. 231. ... 226. 243. 201.]\n",
      "   [254. 254. 255. ...   0.   5.  13.]\n",
      "   ...\n",
      "   [155. 180. 196. ... 137. 153. 126.]\n",
      "   [211. 165. 190. ...   0.   0.   0.]\n",
      "   [173. 192.  41. ... 204. 164. 190.]]\n",
      "\n",
      "  [[155. 180. 195. ... 159. 176. 141.]\n",
      "   [211. 165. 190. ...   0.   0.   1.]\n",
      "   [175. 190.  70. ... 208. 165. 190.]\n",
      "   ...\n",
      "   [ 40.  18.  21. ...   0.   0.   1.]\n",
      "   [ 28.  31.  17. ...  69.  36.  39.]\n",
      "   [ 26.  31.  39. ...  34.  36.  21.]]]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "img = '/home/sldev1/Desktop/Screenshot from 2023-01-19 15-29-04.png'\n",
    "\n",
    "cap = cv2.VideoCapture(img)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "    \n",
    "# rgbframe = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# results = face_detection.process(rgbframe)\n",
    "\n",
    "\n",
    "\n",
    "image_arr = cv2.resize(frame,(256,256))\n",
    "print(image_arr.shape)\n",
    "image_arr = image_arr.reshape(1,3,256,256)\n",
    "print(image_arr.shape)\n",
    "print(image_arr)\n",
    "image_arr = image_arr[...,::-1].astype(\"float\")\n",
    "print(image_arr)\n",
    "image_arr = image_arr/255.\n",
    "print(type(image_arr))\n",
    "\n",
    "# print(type(image_arr))\n",
    "# image_tensor = torch.Tensor(image_arr)\n",
    "# print(type(image_tensor))\n",
    "\n",
    "# out = emo_model(image_tensor)\n",
    "# expr = out['expression']\n",
    "# expr = np.argmax(expr.cpu().detach().numpy(), axis=1)\n",
    "# # expr = np.argmax(expr.cpu().numpy(), axis=1)\n",
    "# print(expr) # type = numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbf02c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.17254902 0.10980392 0.05882353 ... 1.         0.98039216\n",
      "    1.        ]\n",
      "   [1.         0.98039216 1.         ... 0.78039216 0.96862745\n",
      "    0.88235294]\n",
      "   [0.78431373 0.96470588 0.88235294 ... 0.9372549  0.85098039\n",
      "    0.72156863]\n",
      "   ...\n",
      "   [0.98039216 0.92941176 0.99607843 ... 0.76078431 0.9254902\n",
      "    0.85490196]\n",
      "   [0.76470588 0.92156863 0.85098039 ... 0.90588235 0.82745098\n",
      "    0.70588235]\n",
      "   [0.05098039 0.01960784 0.         ... 1.         0.99607843\n",
      "    1.        ]]\n",
      "\n",
      "  [[1.         0.99607843 1.         ... 0.78039216 0.95294118\n",
      "    0.88627451]\n",
      "   [0.78823529 0.95294118 0.88627451 ... 0.90588235 0.82745098\n",
      "    0.70588235]\n",
      "   [0.05098039 0.01960784 0.         ... 1.         0.99607843\n",
      "    0.99607843]\n",
      "   ...\n",
      "   [0.49411765 0.6        0.5372549  ... 0.76862745 0.70588235\n",
      "    0.60784314]\n",
      "   [0.         0.         0.         ... 0.74509804 0.64705882\n",
      "    0.82745098]\n",
      "   [0.74509804 0.64313725 0.8        ... 0.16078431 0.75294118\n",
      "    0.67843137]]\n",
      "\n",
      "  [[0.55294118 0.69019608 0.62352941 ... 0.76470588 0.70588235\n",
      "    0.60784314]\n",
      "   [0.00392157 0.         0.         ... 0.74509804 0.64705882\n",
      "    0.82745098]\n",
      "   [0.74509804 0.64705882 0.81568627 ... 0.2745098  0.74509804\n",
      "    0.68627451]\n",
      "   ...\n",
      "   [0.00392157 0.         0.         ... 0.08235294 0.07058824\n",
      "    0.15686275]\n",
      "   [0.15294118 0.14117647 0.27058824 ... 0.06666667 0.12156863\n",
      "    0.10980392]\n",
      "   [0.08235294 0.14117647 0.13333333 ... 0.15294118 0.12156863\n",
      "    0.10196078]]]]\n"
     ]
    }
   ],
   "source": [
    "image_arr = image_arr[...,::-1]\n",
    "print(image_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40a406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8257ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e8975c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712becc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac0dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfa903cd",
   "metadata": {},
   "source": [
    "## 수정코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5689b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-hs_1wiee because the default path (/home/sldev1/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "2023-01-19 16:07:14.912295: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/sldev1/anaconda3/envs/dms2/lib/python3.7/site-packages/cv2/../../lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.0/lib64:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-01-19 16:07:14.912318: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/sldev1/anaconda3/envs/dms2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from emotion_torch_mobilenetV2 import Emotion\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497f62b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/sldev1/T7/DMS2/08. 정승아_1214.mp4', '/media/sldev1/T7/DMS2/09. 류채원_1214.mp4']\n"
     ]
    }
   ],
   "source": [
    "dirpath = \"/media/sldev1/T7/DMS2/\"\n",
    "# filepath = [\"/media/sldev1/T7/02. 김두용_1205_test.mp4\", '/media/sldev1/T7/03. 주용현_1206_test.mp4']\n",
    "filepath = glob.glob(dirpath+'*.mp4')\n",
    "filepath = sorted(filepath ,key=lambda s: int(re.findall(r'\\d+', s)[3]))\n",
    "filepath = filepath[7:9]\n",
    "print(filepath)\n",
    "emo = Emotion()\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bbf689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/2 [00:00<?, ?it/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/sldev1/T7/DMS2/08. 정승아_1214.mp4\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(filepath):\n",
    "    print(i)\n",
    "\n",
    "    cap = cv2.VideoCapture(i)\n",
    "    with mp_face_detection.FaceDetection(\n",
    "        model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "\n",
    "        with open('/home/sldev1/github/TIL/Internship/DMS2_SMU/FER_Result_MobilenetV2/'+str(i)[22:-4]+'.csv', 'w', newline = '') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"frame\", \"result\"])\n",
    "\n",
    "            while(cap.isOpened()):\n",
    "                ret, frame = cap.read()\n",
    "                rgbframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = face_detection.process(rgbframe)\n",
    "                rgbframe.flags.writeable = True\n",
    "                rgbframe = cv2.cvtColor(rgbframe, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                if results.detections:\n",
    "                    bbox = results.detections[0].location_data.relative_bounding_box\n",
    "                    top = rgbframe.shape[0]*bbox.ymin\n",
    "                    bottom = rgbframe.shape[0]*(bbox.xmin+bbox.height)\n",
    "                    left = rgbframe.shape[1]*bbox.xmin\n",
    "                    right = rgbframe.shape[1]*(bbox.xmin+bbox.width)\n",
    "\n",
    "                    pre_emo = emo.process(frame[int(left)-10:int(right)+10, int(top)-10:int(bottom)+10])\n",
    "                    # 0: neutral, 1: joy, 2: sadness, 3: surprise, 4: fear, 5: disgust, 6: anger, 7: contemp\n",
    "                    cv2.putText(frame, f\"Emotion : {pre_emo}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 200, 0), 2)\n",
    "                    cv2.putText(frame, f\"Frame : {cap.get(1)}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 200), 2)\n",
    "#                     cv2.putText(frame, f\"One : {one}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 200, 0), 2)\n",
    "#                     cv2.putText(frame, f\"two : {two}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 200, 0), 2)\n",
    "\n",
    "\n",
    "                    cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (255, 255, 0), 3)\n",
    "\n",
    "                cv2.imshow(\"frame\", frame)\n",
    "\n",
    "                if cv2.waitKey(1) == ord('q'):\n",
    "                    break\n",
    "\n",
    "                with open('/home/sldev1/github/TIL/Internship/DMS2_SMU/FER_Result_MobilenetV2/'+str(i)[22:-4]+'.csv', 'a', newline = '') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([cap.get(1), pre_emo])\n",
    "                    file.close()\n",
    "                \n",
    "                if cap.get(1) == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "                    break\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# 1258 frame에서 멈추는 중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c0ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fcdc006",
   "metadata": {},
   "source": [
    "## 01/13(금) 오류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46060ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "  6%|████▌                                                                      | 3/50 [2:52:17<45:43:23, 3502.20s/it]\n",
    "/media/sldev1/T7/DMS2/04. 손하늘_1208.mp4\n",
    "[h264 @ 0x559e98b11500] Invalid NAL unit size (7082 > 1541).\n",
    "[h264 @ 0x559e98b11500] Error splitting the input into NAL units.\n",
    "  6%|████▌                                                                      | 3/50 [2:54:15<45:30:00, 3485.13s/it]\n",
    "---------------------------------------------------------------------------\n",
    "error                                     Traceback (most recent call last)\n",
    "/tmp/ipykernel_17891/1244803589.py in <module>\n",
    "     12             while(cap.isOpened()):\n",
    "     13                 ret, frame = cap.read()\n",
    "---> 14                 rgbframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "     15                 results = face_detection.process(rgbframe)\n",
    "     16                 rgbframe.flags.writeable = True\n",
    "\n",
    "error: OpenCV(4.5.5) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
    "                \n",
    "# Frame 2566에서 에러뜸.\n",
    "# 다시 하니까 또 됨. 머노?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55908a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28a2bcbe",
   "metadata": {},
   "source": [
    "## 01/18 (수) csv file editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88e911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sldev1/github/TIL/Internship/DMS2_SMU'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "587087e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a282ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./FER_Result_SL/01. 김형민_1201.csv', './FER_Result_SL/02. 김두용_1205.csv']\n"
     ]
    }
   ],
   "source": [
    "dirpath = \"./FER_Result_SL/\"\n",
    "# filepath = [\"/media/sldev1/T7/02. 김두용_1205_test.mp4\", '/media/sldev1/T7/03. 주용현_1206_test.mp4']\n",
    "filepath = glob.glob(dirpath+'*.csv')\n",
    "filepath = sorted(filepath ,key=lambda s: int(re.findall(r'\\d+', s)[0]))\n",
    "filepath = filepath[0:2]\n",
    "print(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b30227e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_csv.reader' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9824/3866381195.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrow0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrow0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Valence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_csv.reader' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "v = open(filepath[0])\n",
    "r = csv.reader(v)\n",
    "row0 = r.next()\n",
    "row0.append('Valence')\n",
    "print(row0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5b142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535a5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5a2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[py3.7-tf2.3.1]",
   "language": "python",
   "name": "dms2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
