{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea6d909",
   "metadata": {},
   "source": [
    "# 2023-01-19 13:30\n",
    "민규사원님이 주신 mobilenet v2 model 기반으로 다시 돌려보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828dfbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "from torchvision.transforms import transforms\n",
    "# print(torchvision.__version)\n",
    "\n",
    "\n",
    "\n",
    "from mobilenetv2 import mobilenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e47b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e1d1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_model = mobilenet_v2(in_channels=3, num_classes=8) \n",
    "model_path = './mobilenet_v2_affectnet_2023_01_12_07.37'\n",
    "\n",
    "state = torch.load(model_path, map_location='cpu')\n",
    "emo_model.load_state_dict(state['net'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e6afd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(694, 1279, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"transform = transforms.Compose([\\n                        transforms.ToTensor(),])\\n\\n\\nimg = transform(frame)\\n# img = unsqueeze(img, dim=0)\\nout = emo_model(img.unsqueeze(0))\\nprint(out)\\nprint('*********************************************************************************\\n')\\n\\nexpr = out['expression']\\nprint(expr)\\nprint('*********************************************************************************\\n')\\n\\n\\n\\nexpr = np.argmax(expr.cpu().detach().numpy(), axis=1)\\nprint(expr.item())\\nprint('*********************************************************************************\\n')\\n\\n# print(expr) # type = numpy\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '/home/sldev1/Desktop/Screenshot from 2023-01-19 15-29-04.png'\n",
    "\n",
    "cap = cv2.VideoCapture(img)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "print(frame.shape)\n",
    "\n",
    "\n",
    "'''transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "img = transform(frame)\n",
    "# img = unsqueeze(img, dim=0)\n",
    "out = emo_model(img.unsqueeze(0))\n",
    "print(out)\n",
    "print('*********************************************************************************\\n')\n",
    "\n",
    "expr = out['expression']\n",
    "print(expr)\n",
    "print('*********************************************************************************\\n')\n",
    "\n",
    "\n",
    "\n",
    "expr = np.argmax(expr.cpu().detach().numpy(), axis=1)\n",
    "print(expr.item())\n",
    "print('*********************************************************************************\\n')\n",
    "\n",
    "# print(expr) # type = numpy'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "041a07ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21341/4260751373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ebf29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cda65de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0 (default, Oct  9 2018, 10:31:47) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60a1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc434b4a",
   "metadata": {},
   "source": [
    "## 수정코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5689b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-_rsy432w because the default path (/home/sldev1/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "# import tensorflow as tf\n",
    "from emotion_torch_mobilenetV2 import Emotion\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497f62b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "dirpath = \"/media/sldev1/T7/DMS2/\"\n",
    "# filepath = [\"/media/sldev1/T7/02. 김두용_1205_test.mp4\", '/media/sldev1/T7/03. 주용현_1206_test.mp4']\n",
    "filepath = glob.glob(dirpath+'*.mp4')\n",
    "filepath = sorted(filepath ,key=lambda s: int(re.findall(r'\\d+', s)[3]))\n",
    "filepath = filepath[7:9]\n",
    "print(filepath)\n",
    "emo = Emotion()\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93bbf689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(filepath):\n",
    "    print(i)\n",
    "\n",
    "    cap = cv2.VideoCapture(i)\n",
    "    with mp_face_detection.FaceDetection(\n",
    "        model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "\n",
    "        with open('/home/sldev1/github/TIL/Internship/DMS2_SMU/FER_Result_MobilenetV2/'+str(i)[22:-4]+'.csv', 'w', newline = '') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"frame\", \"result\"])\n",
    "\n",
    "            while(cap.isOpened()):\n",
    "                ret, frame = cap.read()\n",
    "                rgbframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = face_detection.process(rgbframe)\n",
    "                rgbframe.flags.writeable = True\n",
    "                rgbframe = cv2.cvtColor(rgbframe, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                if results.detections:\n",
    "                    bbox = results.detections[0].location_data.relative_bounding_box\n",
    "                    top = rgbframe.shape[0]*bbox.ymin\n",
    "                    bottom = rgbframe.shape[0]*(bbox.xmin+bbox.height)\n",
    "                    left = rgbframe.shape[1]*bbox.xmin\n",
    "                    right = rgbframe.shape[1]*(bbox.xmin+bbox.width)\n",
    "\n",
    "                    #pre_emo = emo.process(frame[int(left)-10:int(right)+10, int(top)-10:int(bottom)+10])\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#                     print(np.shape(frame))q\n",
    "                    frame = frame[int(top)-50:int(bottom)+50, int(left)-50:int(right)+20]\n",
    "#                     frame = frame[int(top):int(bottom), int(left):int(right)]\n",
    "#                     print(np.shape(frame))\n",
    "                    frame = cv2.resize(frame, (256, 256))\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "                    pre_emo = emo.process(frame)\n",
    "                    # 0: neutral, 1: joy, 2: sadness, 3: surprise, 4: fear, 5: disgust, 6: anger, 7: contemp\n",
    "                    cv2.putText(frame, f\"Emotion : {pre_emo}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 200, 0), 2)\n",
    "                    cv2.putText(frame, f\"Frame : {cap.get(1)}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 200), 2)\n",
    "#                     cv2.putText(frame, f\"One : {one}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 200, 0), 2)\n",
    "#                     cv2.putText(frame, f\"two : {two}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 200, 0), 2)\n",
    "\n",
    "\n",
    "                    cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (255, 255, 0), 3)\n",
    "\n",
    "                cv2.imshow(\"frame\", frame)\n",
    "\n",
    "                if cv2.waitKey(1) == ord('q'):\n",
    "                    break\n",
    "\n",
    "                with open('/home/sldev1/github/TIL/Internship/DMS2_SMU/FER_Result_MobilenetV2/'+str(i)[22:-4]+'.csv', 'a', newline = '') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([cap.get(1), pre_emo])\n",
    "                    file.close()\n",
    "                \n",
    "                if cap.get(1) == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "                    break\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# 1258 frame에서 멈추는 중"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcdc006",
   "metadata": {},
   "source": [
    "## 01/13(금) 오류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46060ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "  6%|████▌                                                                      | 3/50 [2:52:17<45:43:23, 3502.20s/it]\n",
    "/media/sldev1/T7/DMS2/04. 손하늘_1208.mp4\n",
    "[h264 @ 0x559e98b11500] Invalid NAL unit size (7082 > 1541).\n",
    "[h264 @ 0x559e98b11500] Error splitting the input into NAL units.\n",
    "  6%|████▌                                                                      | 3/50 [2:54:15<45:30:00, 3485.13s/it]\n",
    "---------------------------------------------------------------------------\n",
    "error                                     Traceback (most recent call last)\n",
    "/tmp/ipykernel_17891/1244803589.py in <module>\n",
    "     12             while(cap.isOpened()):\n",
    "     13                 ret, frame = cap.read()\n",
    "---> 14                 rgbframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "     15                 results = face_detection.process(rgbframe)\n",
    "     16                 rgbframe.flags.writeable = True\n",
    "\n",
    "error: OpenCV(4.5.5) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
    "                \n",
    "# Frame 2566에서 에러뜸.\n",
    "# 다시 하니까 또 됨. 머노?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55908a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28a2bcbe",
   "metadata": {},
   "source": [
    "## 01/18 (수) csv file editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88e911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sldev1/github/TIL/Internship/DMS2_SMU'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "587087e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a282ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./FER_Result_SL/01. 김형민_1201.csv', './FER_Result_SL/02. 김두용_1205.csv']\n"
     ]
    }
   ],
   "source": [
    "dirpath = \"./FER_Result_SL/\"\n",
    "# filepath = [\"/media/sldev1/T7/02. 김두용_1205_test.mp4\", '/media/sldev1/T7/03. 주용현_1206_test.mp4']\n",
    "filepath = glob.glob(dirpath+'*.csv')\n",
    "filepath = sorted(filepath ,key=lambda s: int(re.findall(r'\\d+', s)[0]))\n",
    "filepath = filepath[0:2]\n",
    "print(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b30227e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_csv.reader' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9824/3866381195.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrow0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrow0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Valence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_csv.reader' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "v = open(filepath[0])\n",
    "r = csv.reader(v)\n",
    "row0 = r.next()\n",
    "row0.append('Valence')\n",
    "print(row0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5b142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535a5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5a2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[py3.7.15]",
   "language": "python",
   "name": "py3.7.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
