{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba52841a-c25f-46cb-be22-3ba52825ea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset_Image 개수 : 3999\n",
      "Test Dataset_Annotation 개수 : 15996\n",
      "**************\n",
      "\n",
      "***************\n",
      "Train Dataset_Image 개수 : 287651\n",
      "Train Dataset_Annotation 개수 : 1150604\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_path = '/home/sldev1/Project/hyeongeun_test/data/FER/'\n",
    "test_img_ = len(os.listdir(data_path+\"val_set/images/\"))\n",
    "test_ano_ = len(os.listdir(data_path+\"val_set/annotations/\"))\n",
    "train_img_ = len(os.listdir(data_path+\"train_set/images/\"))\n",
    "train_ano_ = len(os.listdir(data_path+\"train_set/annotations/\"))\n",
    "\n",
    "\n",
    "print(f\"Test Dataset_Image 개수 : {test_img_}\")\n",
    "print(f\"Test Dataset_Annotation 개수 : {test_ano_}\")\n",
    "print(\"**************\\n\\n***************\")\n",
    "print(f\"Train Dataset_Image 개수 : {train_img_}\")\n",
    "print(f\"Train Dataset_Annotation 개수 : {train_ano_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28fe1b-ffb0-4568-9b35-33af0d9d5573",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 22.11.01 (화)\n",
    "### 느낀점 : 데이터 전처리 힘들게 완료\n",
    "1. Custom Dataset 안에 init에는 경로만 들어가면 된다.   \n",
    "2. getitem 인덱스는 하나씩 꺼내먹을 수 도 있고, dataloader에서 batch묶음으로 들어가기도 하는 존재다.\n",
    "\n",
    "----\n",
    "# 11.02 (수) 해야하는 것\n",
    "1. 데이터 전처리 다했으니, 이제 학습하는 코드 짜기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c50d0bdf-7501-4e19-af08-b1267744ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "\n",
    "\n",
    "def img_load(path, train=True):\n",
    "    if train:\n",
    "        img_path = path+'/train_set/images/'\n",
    "    else:\n",
    "        img_path = path+'/val_set/images/'\n",
    "\n",
    "    jpg = glob.glob(img_path+'*.jpg')\n",
    "    sort_jpg = sorted(jpg, key=lambda s: int(re.findall(r'\\d+', s)[1]))\n",
    "    return sort_jpg\n",
    "    #print (\"img_list_jpg: {}\".format(img_list_jpg))\n",
    "    #print (\"img_list_jpg: {}\".format(sort_img_list_jpg))\n",
    "\n",
    "\n",
    "def label_load(path, train=True):\n",
    "    if train:\n",
    "        label_path = path+'/train_set/annotations/'\n",
    "    else:\n",
    "        label_path = path+'/val_set/annotations/'\n",
    "\n",
    "    label = glob.glob(label_path+'*exp.npy')\n",
    "    #label\n",
    "    sort_label = sorted(label,key=lambda s: int(re.findall(r'\\d+', s)[1]))\n",
    "    return sort_label\n",
    "    \n",
    "\n",
    "class MyFER_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_data, label_data, train=True, transform=None):\n",
    "        self.img = img_load(img_data)\n",
    "        self.label = label_load(label_data)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        self.img = Image.open(self.img[idx]) \n",
    "        self.label = np.array(self.label[idx])\n",
    "        sample =  self.img\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "4bced986-5115-4d14-9985-937b0318a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3999,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x = label_load(path = data_path, train=False)\n",
    "x = np.array(x)\n",
    "print(x.shape)\n",
    "print(type(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "720619d3-deaf-4361-9761-9f69cbf274d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import PIL\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as tr\n",
    "\n",
    "path = \"/home/sldev1/Project/hyeongeun_test/data/FER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "5cf3ee27-150b-4122-a37e-77b153945fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = tr.Compose([tr.ToTensor(),\n",
    "                              tr.Normalize])\n",
    "transform_test = tr.Compose([tr.ToTensor()])\n",
    "\n",
    "\n",
    "train_dataset = MyFER_Dataset(img_data = path,\n",
    "                              label_data = path,\n",
    "                              train=True,\n",
    "                              transform=transform_train)\n",
    "test_dataset = MyFER_Dataset(img_data = path,\n",
    "                              label_data = path,\n",
    "                              train=False,\n",
    "                              transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4ec854e9-ee32-4b40-ab14-f07e6099dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5d5ea4b2-abef-4c52-9a33-64b57165a2be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6667, 0.6902, 0.6863,  ..., 0.7451, 0.7490, 0.7529],\n",
       "         [0.6941, 0.6863, 0.6627,  ..., 0.7412, 0.7490, 0.7529],\n",
       "         [0.7098, 0.6627, 0.6235,  ..., 0.7412, 0.7490, 0.7529],\n",
       "         ...,\n",
       "         [0.0275, 0.0157, 0.0196,  ..., 0.5294, 0.5294, 0.5333],\n",
       "         [0.0314, 0.0196, 0.0157,  ..., 0.5294, 0.5294, 0.5255],\n",
       "         [0.0353, 0.0196, 0.0157,  ..., 0.5294, 0.5255, 0.5176]],\n",
       "\n",
       "        [[0.5843, 0.6078, 0.6078,  ..., 0.6980, 0.7020, 0.7059],\n",
       "         [0.6118, 0.6039, 0.5804,  ..., 0.6941, 0.7020, 0.7059],\n",
       "         [0.6275, 0.5804, 0.5412,  ..., 0.6941, 0.7020, 0.7059],\n",
       "         ...,\n",
       "         [0.0392, 0.0275, 0.0314,  ..., 0.5412, 0.5412, 0.5451],\n",
       "         [0.0431, 0.0314, 0.0275,  ..., 0.5412, 0.5412, 0.5373],\n",
       "         [0.0471, 0.0314, 0.0275,  ..., 0.5412, 0.5373, 0.5294]],\n",
       "\n",
       "        [[0.3608, 0.3843, 0.3765,  ..., 0.4941, 0.4980, 0.5020],\n",
       "         [0.3882, 0.3804, 0.3569,  ..., 0.4902, 0.4980, 0.5020],\n",
       "         [0.4118, 0.3569, 0.3176,  ..., 0.4902, 0.4980, 0.5020],\n",
       "         ...,\n",
       "         [0.0588, 0.0471, 0.0510,  ..., 0.3725, 0.3725, 0.3765],\n",
       "         [0.0627, 0.0510, 0.0471,  ..., 0.3725, 0.3725, 0.3686],\n",
       "         [0.0667, 0.0510, 0.0471,  ..., 0.3725, 0.3686, 0.3608]]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567768f2-4c30-4263-9add-44f865e56348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed1ca4-8bb1-4d9d-ae95-37ed05ab92dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
