{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG 논문 구현 with CIFAR10\n",
    "https://cryptosalamander.tistory.com/158   \n",
    "이 친구 resnet도 있음.   \n",
    "내일 출근(10/24)해서 전체코드 한번 싹 훑고   \n",
    "resnet하고 MobileNet 찾아 보고 하자.   \n",
    "하루에 네트워크 하나씩 하면 될 듯?!   \n",
    "    \n",
    "       \n",
    "\n",
    "-----\n",
    "2022/10/24 발견한 것.    \n",
    "랜덤시드 고정 안해놔서 copy후 retrain 시키겠음.    \n",
    "\n",
    "-----\n",
    "### tensorboard를 통한 학습 현황 확인하기\n",
    "#### └─11월 8일\n",
    "- 학습하는 과정을 시각화한다랄까~!?   \n",
    "\n",
    "\n",
    "# [여서 시작하니까 클릭 하거라잉](#여기서-부터-시작)   \n",
    "\n",
    "아래 링크   \n",
    "https://gaussian37.github.io/dl-pytorch-observe/#tensorboard%EB%A5%BC-%ED%86%B5%ED%95%9C-%ED%95%99%EC%8A%B5-%ED%98%84%ED%99%A9-%ED%99%95%EC%9D%B8-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from VGG import VGG11, VGG13, VGG16, VGG19\n",
    "import os\n",
    "import torchvision.models as models\n",
    "\n",
    "''' random seed fixed'''\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Learning Rate Scheduler\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch == 30:\n",
    "        lr /= 2\n",
    "    if epoch == 60:\n",
    "        lr /= 2\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr \n",
    "\n",
    "# Xavier         \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_uniform(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='/home/sldev1/Project/hyeongeun_test/data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='/home/sldev1/Project/hyeongeun_test/data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = VGG16(cifar=True)\n",
    "# VGG11, VGG13, VGG16, VGG19 중에 택일하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model.apply(init_weights)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "num_epoch = 50\n",
    "model_name = '11.08.model.pth'\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "train_loss = 0\n",
    "valid_loss = 0\n",
    "correct = 0\n",
    "total_cnt = 0\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== 1 epoch of 50 ======\n",
      "\n",
      "====== 100 Step of 391 ======\n",
      "Train Acc : 0.09777227722772278\n",
      "Train Loss : 0.01797320693731308\n",
      "\n",
      "====== 200 Step of 391 ======\n",
      "Train Acc : 0.09868625621890548\n",
      "Train Loss : 0.017977828159928322\n",
      "\n",
      "====== 300 Step of 391 ======\n",
      "Train Acc : 0.09857765780730897\n",
      "Train Loss : 0.01799503155052662\n",
      "\n",
      "Valid Acc : 0.1\n",
      "Valid Loss : 0.018194081261754036\n",
      "Model Saved!\n",
      "====== 2 epoch of 50 ======\n",
      "\n",
      "====== 100 Step of 391 ======\n",
      "Train Acc : 0.09761757425742575\n",
      "Train Loss : 0.017982928082346916\n",
      "\n",
      "====== 200 Step of 391 ======\n",
      "Train Acc : 0.09705379353233831\n",
      "Train Loss : 0.01799253188073635\n",
      "\n",
      "====== 300 Step of 391 ======\n",
      "Train Acc : 0.09844788205980066\n",
      "Train Loss : 0.018004903569817543\n",
      "\n",
      "Valid Acc : 0.1\n",
      "Valid Loss : 0.01820112019777298\n",
      "====== 3 epoch of 50 ======\n",
      "\n",
      "====== 100 Step of 391 ======\n",
      "Train Acc : 0.09823638613861387\n",
      "Train Loss : 0.01799100823700428\n",
      "\n",
      "====== 200 Step of 391 ======\n",
      "Train Acc : 0.1000077736318408\n",
      "Train Loss : 0.017969638109207153\n",
      "\n",
      "====== 300 Step of 391 ======\n",
      "Train Acc : 0.09940822259136213\n",
      "Train Loss : 0.017993757501244545\n",
      "\n",
      "Valid Acc : 0.1\n",
      "Valid Loss : 0.018199026584625244\n",
      "====== 4 epoch of 50 ======\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7c82163f49bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "writer = SummaryWriter()\n",
    "for epoch in range(num_epoch):\n",
    "    print(f\"====== { epoch+1} epoch of { num_epoch } ======\")\n",
    "    model.train()\n",
    "    lr_scheduler(optimizer, epoch)\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    total_cnt = 0\n",
    "    \n",
    "    \n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    #writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', valid_loss, epoch)\n",
    "    #writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n",
    "    \n",
    "    # Train Phase\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        #  input and target\n",
    "        batch[0], batch[1] = batch[0].to(device), batch[1].to(device) # batch[0]은 이미지 / batch[1]은 label\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(batch[0])\n",
    "        loss = loss_fn(logits, batch[1])\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predict = logits.max(1)\n",
    "        \n",
    "        total_cnt += batch[1].size(0)\n",
    "        correct +=  predict.eq(batch[1]).sum().item()\n",
    "        \n",
    "        \n",
    "        if step % 100 == 0 and step != 0:\n",
    "            print(f\"\\n====== { step } Step of { len(train_loader) } ======\")\n",
    "            print(f\"Train Acc : { correct / total_cnt }\")\n",
    "            print(f\"Train Loss : { loss.item() / batch[1].size(0) }\")\n",
    "        \n",
    "            \n",
    "    correct = 0\n",
    "    total_cnt = 0\n",
    "    \n",
    "# Test Phase\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(test_loader):\n",
    "            # input and target\n",
    "            batch[0], batch[1] = batch[0].to(device), batch[1].to(device)\n",
    "            total_cnt += batch[1].size(0)\n",
    "            logits = model(batch[0])\n",
    "            valid_loss += loss_fn(logits, batch[1])\n",
    "            _, predict = logits.max(1)\n",
    "            correct += predict.eq(batch[1]).sum().item()\n",
    "        valid_acc = correct / total_cnt\n",
    "        print(f\"\\nValid Acc : { valid_acc }\")    \n",
    "        print(f\"Valid Loss : { valid_loss / total_cnt }\")\n",
    "\n",
    "        if(valid_acc > best_acc):\n",
    "            best_acc = valid_acc\n",
    "            #torch.save(model, model_name)\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(\"Model Saved!\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기서 부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "x = range(100)\n",
    "for i in x:\n",
    "    writer.add_scalar('y=2x', i * 2, i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.11.3)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (59.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.7.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
