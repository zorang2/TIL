{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9d6531-9a52-4eea-866c-f419707cb293",
   "metadata": {},
   "source": [
    "# Model 시각화 및 학습 현황 확인 방법 정리.\n",
    "https://gaussian37.github.io/dl-pytorch-observe/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13587a30-5543-44a3-8df1-5d6213292d5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. torchsummary\n",
    "\n",
    "\n",
    "| \\ |torch 제공 ResNet|Custom ResNet|\n",
    "|:------|---:|---:|\n",
    "|Total params|25,557,032|23,528,586|\n",
    "|Forward/backward pass size (MB)|351.15|251.68|\n",
    "|Params size (MB)|97.49|89.75|\n",
    "|Estimated Total Size (MB)|449.33|342.12|\n",
    "\n",
    "차이 안날 줄 알았는데 조~금 차이나네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e38ca88-87b3-4eb1-b479-ff3cfb689c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b0f7ab-8879-40e2-a6e9-0425ebb399d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 122, 122]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 122, 122]             128\n",
      "              ReLU-3         [-1, 64, 122, 122]               0\n",
      "         MaxPool2d-4           [-1, 64, 61, 61]               0\n",
      "            Conv2d-5           [-1, 64, 61, 61]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 61, 61]             128\n",
      "              ReLU-7           [-1, 64, 61, 61]               0\n",
      "            Conv2d-8           [-1, 64, 61, 61]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 61, 61]             128\n",
      "             ReLU-10           [-1, 64, 61, 61]               0\n",
      "           Conv2d-11          [-1, 256, 61, 61]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 61, 61]             512\n",
      "           Conv2d-13          [-1, 256, 61, 61]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 61, 61]             512\n",
      "             ReLU-15          [-1, 256, 61, 61]               0\n",
      "       Bottleneck-16          [-1, 256, 61, 61]               0\n",
      "           Conv2d-17           [-1, 64, 61, 61]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 61, 61]             128\n",
      "             ReLU-19           [-1, 64, 61, 61]               0\n",
      "           Conv2d-20           [-1, 64, 61, 61]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 61, 61]             128\n",
      "             ReLU-22           [-1, 64, 61, 61]               0\n",
      "           Conv2d-23          [-1, 256, 61, 61]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 61, 61]             512\n",
      "             ReLU-25          [-1, 256, 61, 61]               0\n",
      "       Bottleneck-26          [-1, 256, 61, 61]               0\n",
      "           Conv2d-27           [-1, 64, 61, 61]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 61, 61]             128\n",
      "             ReLU-29           [-1, 64, 61, 61]               0\n",
      "           Conv2d-30           [-1, 64, 61, 61]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 61, 61]             128\n",
      "             ReLU-32           [-1, 64, 61, 61]               0\n",
      "           Conv2d-33          [-1, 256, 61, 61]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 61, 61]             512\n",
      "             ReLU-35          [-1, 256, 61, 61]               0\n",
      "       Bottleneck-36          [-1, 256, 61, 61]               0\n",
      "           Conv2d-37          [-1, 128, 61, 61]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 61, 61]             256\n",
      "             ReLU-39          [-1, 128, 61, 61]               0\n",
      "           Conv2d-40          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 31, 31]             256\n",
      "             ReLU-42          [-1, 128, 31, 31]               0\n",
      "           Conv2d-43          [-1, 512, 31, 31]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 31, 31]           1,024\n",
      "           Conv2d-45          [-1, 512, 31, 31]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 31, 31]           1,024\n",
      "             ReLU-47          [-1, 512, 31, 31]               0\n",
      "       Bottleneck-48          [-1, 512, 31, 31]               0\n",
      "           Conv2d-49          [-1, 128, 31, 31]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 31, 31]             256\n",
      "             ReLU-51          [-1, 128, 31, 31]               0\n",
      "           Conv2d-52          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 31, 31]             256\n",
      "             ReLU-54          [-1, 128, 31, 31]               0\n",
      "           Conv2d-55          [-1, 512, 31, 31]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 31, 31]           1,024\n",
      "             ReLU-57          [-1, 512, 31, 31]               0\n",
      "       Bottleneck-58          [-1, 512, 31, 31]               0\n",
      "           Conv2d-59          [-1, 128, 31, 31]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 31, 31]             256\n",
      "             ReLU-61          [-1, 128, 31, 31]               0\n",
      "           Conv2d-62          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 31, 31]             256\n",
      "             ReLU-64          [-1, 128, 31, 31]               0\n",
      "           Conv2d-65          [-1, 512, 31, 31]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 31, 31]           1,024\n",
      "             ReLU-67          [-1, 512, 31, 31]               0\n",
      "       Bottleneck-68          [-1, 512, 31, 31]               0\n",
      "           Conv2d-69          [-1, 128, 31, 31]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 31, 31]             256\n",
      "             ReLU-71          [-1, 128, 31, 31]               0\n",
      "           Conv2d-72          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 31, 31]             256\n",
      "             ReLU-74          [-1, 128, 31, 31]               0\n",
      "           Conv2d-75          [-1, 512, 31, 31]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 31, 31]           1,024\n",
      "             ReLU-77          [-1, 512, 31, 31]               0\n",
      "       Bottleneck-78          [-1, 512, 31, 31]               0\n",
      "           Conv2d-79          [-1, 256, 31, 31]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 31, 31]             512\n",
      "             ReLU-81          [-1, 256, 31, 31]               0\n",
      "           Conv2d-82          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 16, 16]             512\n",
      "             ReLU-84          [-1, 256, 16, 16]               0\n",
      "           Conv2d-85         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 16, 16]           2,048\n",
      "           Conv2d-87         [-1, 1024, 16, 16]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-89         [-1, 1024, 16, 16]               0\n",
      "       Bottleneck-90         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-91          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 16, 16]             512\n",
      "             ReLU-93          [-1, 256, 16, 16]               0\n",
      "           Conv2d-94          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 16, 16]             512\n",
      "             ReLU-96          [-1, 256, 16, 16]               0\n",
      "           Conv2d-97         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 16, 16]           2,048\n",
      "             ReLU-99         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-100         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-101          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 16, 16]             512\n",
      "            ReLU-103          [-1, 256, 16, 16]               0\n",
      "          Conv2d-104          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 16, 16]             512\n",
      "            ReLU-106          [-1, 256, 16, 16]               0\n",
      "          Conv2d-107         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-109         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-110         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-111          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 16, 16]             512\n",
      "            ReLU-113          [-1, 256, 16, 16]               0\n",
      "          Conv2d-114          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 16, 16]             512\n",
      "            ReLU-116          [-1, 256, 16, 16]               0\n",
      "          Conv2d-117         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-119         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-120         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-121          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 16, 16]             512\n",
      "            ReLU-123          [-1, 256, 16, 16]               0\n",
      "          Conv2d-124          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 16, 16]             512\n",
      "            ReLU-126          [-1, 256, 16, 16]               0\n",
      "          Conv2d-127         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-129         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-130         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-131          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 16, 16]             512\n",
      "            ReLU-133          [-1, 256, 16, 16]               0\n",
      "          Conv2d-134          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 16, 16]             512\n",
      "            ReLU-136          [-1, 256, 16, 16]               0\n",
      "          Conv2d-137         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-139         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-140         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-141          [-1, 512, 16, 16]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-143          [-1, 512, 16, 16]               0\n",
      "          Conv2d-144            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-146            [-1, 512, 8, 8]               0\n",
      "          Conv2d-147           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 8, 8]           4,096\n",
      "          Conv2d-149           [-1, 2048, 8, 8]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-151           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-152           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-153            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-155            [-1, 512, 8, 8]               0\n",
      "          Conv2d-156            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-158            [-1, 512, 8, 8]               0\n",
      "          Conv2d-159           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-161           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-162           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-163            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-165            [-1, 512, 8, 8]               0\n",
      "          Conv2d-166            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-168            [-1, 512, 8, 8]               0\n",
      "          Conv2d-169           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-171           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-172           [-1, 2048, 8, 8]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.68\n",
      "Forward/backward pass size (MB): 351.15\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 449.33\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "\n",
    "mymodel = models.resnet50()\n",
    "mymodel.to(device)\n",
    "summary(mymodel, (3, 244, 244))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60b7ba6-a6a8-4adc-b44c-1f0af520783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b993ebf-2ec9-4eb0-8b06-252d2c48b02c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 122, 122]           9,472\n",
      "       BatchNorm2d-2         [-1, 64, 122, 122]             128\n",
      "         MaxPool2d-3           [-1, 64, 61, 61]               0\n",
      "            Conv2d-4           [-1, 64, 61, 61]           4,096\n",
      "       BatchNorm2d-5           [-1, 64, 61, 61]             128\n",
      "            Conv2d-6           [-1, 64, 61, 61]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 61, 61]             128\n",
      "            Conv2d-8          [-1, 256, 61, 61]          16,384\n",
      "       BatchNorm2d-9          [-1, 256, 61, 61]             512\n",
      "           Conv2d-10          [-1, 256, 61, 61]          16,384\n",
      "      BatchNorm2d-11          [-1, 256, 61, 61]             512\n",
      "       BottleNeck-12          [-1, 256, 61, 61]               0\n",
      "           Conv2d-13           [-1, 64, 61, 61]          16,384\n",
      "      BatchNorm2d-14           [-1, 64, 61, 61]             128\n",
      "           Conv2d-15           [-1, 64, 61, 61]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 61, 61]             128\n",
      "           Conv2d-17          [-1, 256, 61, 61]          16,384\n",
      "      BatchNorm2d-18          [-1, 256, 61, 61]             512\n",
      "       BottleNeck-19          [-1, 256, 61, 61]               0\n",
      "           Conv2d-20           [-1, 64, 61, 61]          16,384\n",
      "      BatchNorm2d-21           [-1, 64, 61, 61]             128\n",
      "           Conv2d-22           [-1, 64, 61, 61]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 61, 61]             128\n",
      "           Conv2d-24          [-1, 256, 61, 61]          16,384\n",
      "      BatchNorm2d-25          [-1, 256, 61, 61]             512\n",
      "       BottleNeck-26          [-1, 256, 61, 61]               0\n",
      "           Conv2d-27          [-1, 128, 31, 31]          32,768\n",
      "      BatchNorm2d-28          [-1, 128, 31, 31]             256\n",
      "           Conv2d-29          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 31, 31]             256\n",
      "           Conv2d-31          [-1, 512, 31, 31]          65,536\n",
      "      BatchNorm2d-32          [-1, 512, 31, 31]           1,024\n",
      "           Conv2d-33          [-1, 512, 31, 31]         131,072\n",
      "      BatchNorm2d-34          [-1, 512, 31, 31]           1,024\n",
      "       BottleNeck-35          [-1, 512, 31, 31]               0\n",
      "           Conv2d-36          [-1, 128, 31, 31]          65,536\n",
      "      BatchNorm2d-37          [-1, 128, 31, 31]             256\n",
      "           Conv2d-38          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 31, 31]             256\n",
      "           Conv2d-40          [-1, 512, 31, 31]          65,536\n",
      "      BatchNorm2d-41          [-1, 512, 31, 31]           1,024\n",
      "       BottleNeck-42          [-1, 512, 31, 31]               0\n",
      "           Conv2d-43          [-1, 128, 31, 31]          65,536\n",
      "      BatchNorm2d-44          [-1, 128, 31, 31]             256\n",
      "           Conv2d-45          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 31, 31]             256\n",
      "           Conv2d-47          [-1, 512, 31, 31]          65,536\n",
      "      BatchNorm2d-48          [-1, 512, 31, 31]           1,024\n",
      "       BottleNeck-49          [-1, 512, 31, 31]               0\n",
      "           Conv2d-50          [-1, 128, 31, 31]          65,536\n",
      "      BatchNorm2d-51          [-1, 128, 31, 31]             256\n",
      "           Conv2d-52          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 31, 31]             256\n",
      "           Conv2d-54          [-1, 512, 31, 31]          65,536\n",
      "      BatchNorm2d-55          [-1, 512, 31, 31]           1,024\n",
      "       BottleNeck-56          [-1, 512, 31, 31]               0\n",
      "           Conv2d-57          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-58          [-1, 256, 16, 16]             512\n",
      "           Conv2d-59          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 16, 16]             512\n",
      "           Conv2d-61         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-62         [-1, 1024, 16, 16]           2,048\n",
      "           Conv2d-63         [-1, 1024, 16, 16]         524,288\n",
      "      BatchNorm2d-64         [-1, 1024, 16, 16]           2,048\n",
      "       BottleNeck-65         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-66          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-67          [-1, 256, 16, 16]             512\n",
      "           Conv2d-68          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 16, 16]             512\n",
      "           Conv2d-70         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-71         [-1, 1024, 16, 16]           2,048\n",
      "       BottleNeck-72         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-73          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-74          [-1, 256, 16, 16]             512\n",
      "           Conv2d-75          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 16, 16]             512\n",
      "           Conv2d-77         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-78         [-1, 1024, 16, 16]           2,048\n",
      "       BottleNeck-79         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-80          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-81          [-1, 256, 16, 16]             512\n",
      "           Conv2d-82          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 16, 16]             512\n",
      "           Conv2d-84         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-85         [-1, 1024, 16, 16]           2,048\n",
      "       BottleNeck-86         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-87          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-88          [-1, 256, 16, 16]             512\n",
      "           Conv2d-89          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 16, 16]             512\n",
      "           Conv2d-91         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-92         [-1, 1024, 16, 16]           2,048\n",
      "       BottleNeck-93         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-94          [-1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-95          [-1, 256, 16, 16]             512\n",
      "           Conv2d-96          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 16, 16]             512\n",
      "           Conv2d-98         [-1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-99         [-1, 1024, 16, 16]           2,048\n",
      "      BottleNeck-100         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-101            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-102            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-103            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-104            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-105           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-106           [-1, 2048, 8, 8]           4,096\n",
      "          Conv2d-107           [-1, 2048, 8, 8]       2,097,152\n",
      "     BatchNorm2d-108           [-1, 2048, 8, 8]           4,096\n",
      "      BottleNeck-109           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-110            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-111            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-112            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-114           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-115           [-1, 2048, 8, 8]           4,096\n",
      "      BottleNeck-116           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-117            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-118            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-119            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-120            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-121           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-122           [-1, 2048, 8, 8]           4,096\n",
      "      BottleNeck-123           [-1, 2048, 8, 8]               0\n",
      "AdaptiveAvgPool2d-124           [-1, 2048, 1, 1]               0\n",
      "          Linear-125                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,528,586\n",
      "Trainable params: 23,528,586\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.68\n",
      "Forward/backward pass size (MB): 251.68\n",
      "Params size (MB): 89.75\n",
      "Estimated Total Size (MB): 342.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "\n",
    "mymodel = ResNet50()\n",
    "mymodel.to(device)\n",
    "summary(mymodel, (3, 244, 244))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a1310-1d14-477d-984f-feb321a2b58d",
   "metadata": {},
   "source": [
    "## 2. torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094c8cd-0862-4b3c-93ba-7506411435d8",
   "metadata": {},
   "source": [
    "안타깝게도 앞에서 설명한 torchsummary의 업데이트가 되지 않는 반면 새로운 모델 정보 요약 라이브러리인 torchinfo가 많이 사용되고 있습니다.   \n",
    "-torchinfo는 기존의 torchsummary와 사용 방법은 거의 같습니다.   \n",
    "더구나 기존의 torchsummary에서 LSTM과 같은 RNN 계열의 Summary 시 일부 오류가 났던 문제와 layer 분류를 좀 더 계층적으로 상세히 해준다는 점 등의 개선이 있어서 torchsummary 대신 torchinfo를 사용하는 것을 추천 드립니다.   \n",
    "사용 방법의 일부 차이점은 torchinfo에서는 (batch, channel, height, width)와 같은 형태로 데이터를 입력 받습니다.   \n",
    "이는 torchsummary 에서 batch를 사용하지 않고 (channel, height, width)를 사용하는 것과 차이점입니다.   \n",
    "실제 사용하는 입장에서는 batch를 고려하여 모델을 설계하는 것이 더 현실적이기 때문에 torchinfo를 사용하는 것이 더 좋다고 생각이 듭니다.  \n",
    "아래는 VGG16에 (3, 224, 224) 크기의 이미지를 250개의 batch를 통하여 feedforward 하였을 때의 상태를 요약해 주는 예제 코드 입니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e3c5cdf-8bac-4024-ad89-0f5705c11ce8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.5.4-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.5.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dbf0685-9752-456b-a89c-4191cef06e66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   --                        --\n",
       "├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n",
       "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-5                        [1, 256, 56, 56]          --\n",
       "│    └─Bottleneck: 2-1                   [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           4,096\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-7                  [1, 256, 56, 56]          16,384\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 256, 56, 56]          512\n",
       "│    │    └─Sequential: 3-9              [1, 256, 56, 56]          16,896\n",
       "│    │    └─ReLU: 3-10                   [1, 256, 56, 56]          --\n",
       "│    └─Bottleneck: 2-2                   [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-11                 [1, 64, 56, 56]           16,384\n",
       "│    │    └─BatchNorm2d: 3-12            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-13                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-14                 [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-15            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-16                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-17                 [1, 256, 56, 56]          16,384\n",
       "│    │    └─BatchNorm2d: 3-18            [1, 256, 56, 56]          512\n",
       "│    │    └─ReLU: 3-19                   [1, 256, 56, 56]          --\n",
       "│    └─Bottleneck: 2-3                   [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 64, 56, 56]           16,384\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-22                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-23                 [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-25                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 56, 56]          16,384\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 56, 56]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 56, 56]          --\n",
       "├─Sequential: 1-6                        [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck: 2-4                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 128, 56, 56]          32,768\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 128, 56, 56]          256\n",
       "│    │    └─ReLU: 3-31                   [1, 128, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-32                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-33            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-34                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-35                 [1, 512, 28, 28]          65,536\n",
       "│    │    └─BatchNorm2d: 3-36            [1, 512, 28, 28]          1,024\n",
       "│    │    └─Sequential: 3-37             [1, 512, 28, 28]          132,096\n",
       "│    │    └─ReLU: 3-38                   [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck: 2-5                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-39                 [1, 128, 28, 28]          65,536\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-41                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-42                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-44                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-45                 [1, 512, 28, 28]          65,536\n",
       "│    │    └─BatchNorm2d: 3-46            [1, 512, 28, 28]          1,024\n",
       "│    │    └─ReLU: 3-47                   [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck: 2-6                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-48                 [1, 128, 28, 28]          65,536\n",
       "│    │    └─BatchNorm2d: 3-49            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-50                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-51                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-52            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-53                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-54                 [1, 512, 28, 28]          65,536\n",
       "│    │    └─BatchNorm2d: 3-55            [1, 512, 28, 28]          1,024\n",
       "│    │    └─ReLU: 3-56                   [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck: 2-7                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-57                 [1, 128, 28, 28]          65,536\n",
       "│    │    └─BatchNorm2d: 3-58            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-59                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-60                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-61            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-62                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-63                 [1, 512, 28, 28]          65,536\n",
       "│    │    └─BatchNorm2d: 3-64            [1, 512, 28, 28]          1,024\n",
       "│    │    └─ReLU: 3-65                   [1, 512, 28, 28]          --\n",
       "├─Sequential: 1-7                        [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-8                   [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-66                 [1, 256, 28, 28]          131,072\n",
       "│    │    └─BatchNorm2d: 3-67            [1, 256, 28, 28]          512\n",
       "│    │    └─ReLU: 3-68                   [1, 256, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-69                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-70            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-71                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-72                 [1, 1024, 14, 14]         262,144\n",
       "│    │    └─BatchNorm2d: 3-73            [1, 1024, 14, 14]         2,048\n",
       "│    │    └─Sequential: 3-74             [1, 1024, 14, 14]         526,336\n",
       "│    │    └─ReLU: 3-75                   [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-9                   [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-76                 [1, 256, 14, 14]          262,144\n",
       "│    │    └─BatchNorm2d: 3-77            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-78                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-79                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-80            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-81                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-82                 [1, 1024, 14, 14]         262,144\n",
       "│    │    └─BatchNorm2d: 3-83            [1, 1024, 14, 14]         2,048\n",
       "│    │    └─ReLU: 3-84                   [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-10                  [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-85                 [1, 256, 14, 14]          262,144\n",
       "│    │    └─BatchNorm2d: 3-86            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-87                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-88                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-89            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-90                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-91                 [1, 1024, 14, 14]         262,144\n",
       "│    │    └─BatchNorm2d: 3-92            [1, 1024, 14, 14]         2,048\n",
       "│    │    └─ReLU: 3-93                   [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-11                  [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-94                 [1, 256, 14, 14]          262,144\n",
       "│    │    └─BatchNorm2d: 3-95            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-96                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-97                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-98            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-99                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-100                [1, 1024, 14, 14]         262,144\n",
       "│    │    └─BatchNorm2d: 3-101           [1, 1024, 14, 14]         2,048\n",
       "│    │    └─ReLU: 3-102                  [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-12                  [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-103                [1, 256, 14, 14]          262,144\n",
       "│    │    └─BatchNorm2d: 3-104           [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-105                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-106                [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-107           [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-108                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-109                [1, 1024, 14, 14]         262,144\n",
       "│    │    └─BatchNorm2d: 3-110           [1, 1024, 14, 14]         2,048\n",
       "│    │    └─ReLU: 3-111                  [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-13                  [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-112                [1, 256, 14, 14]          262,144\n",
       "│    │    └─BatchNorm2d: 3-113           [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-114                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-115                [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-116           [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-117                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-118                [1, 1024, 14, 14]         262,144\n",
       "│    │    └─BatchNorm2d: 3-119           [1, 1024, 14, 14]         2,048\n",
       "│    │    └─ReLU: 3-120                  [1, 1024, 14, 14]         --\n",
       "├─Sequential: 1-8                        [1, 2048, 7, 7]           --\n",
       "│    └─Bottleneck: 2-14                  [1, 2048, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-121                [1, 512, 14, 14]          524,288\n",
       "│    │    └─BatchNorm2d: 3-122           [1, 512, 14, 14]          1,024\n",
       "│    │    └─ReLU: 3-123                  [1, 512, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-124                [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-125           [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-126                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-127                [1, 2048, 7, 7]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-128           [1, 2048, 7, 7]           4,096\n",
       "│    │    └─Sequential: 3-129            [1, 2048, 7, 7]           2,101,248\n",
       "│    │    └─ReLU: 3-130                  [1, 2048, 7, 7]           --\n",
       "│    └─Bottleneck: 2-15                  [1, 2048, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-131                [1, 512, 7, 7]            1,048,576\n",
       "│    │    └─BatchNorm2d: 3-132           [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-133                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-134                [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-135           [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-136                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-137                [1, 2048, 7, 7]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-138           [1, 2048, 7, 7]           4,096\n",
       "│    │    └─ReLU: 3-139                  [1, 2048, 7, 7]           --\n",
       "│    └─Bottleneck: 2-16                  [1, 2048, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-140                [1, 512, 7, 7]            1,048,576\n",
       "│    │    └─BatchNorm2d: 3-141           [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-142                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-143                [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-144           [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-145                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-146                [1, 2048, 7, 7]           1,048,576\n",
       "│    │    └─BatchNorm2d: 3-147           [1, 2048, 7, 7]           4,096\n",
       "│    │    └─ReLU: 3-148                  [1, 2048, 7, 7]           --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 2048, 1, 1]           --\n",
       "├─Linear: 1-10                           [1, 1000]                 2,049,000\n",
       "==========================================================================================\n",
       "Total params: 25,557,032\n",
       "Trainable params: 25,557,032\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.09\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 177.83\n",
       "Params size (MB): 102.23\n",
       "Estimated Total Size (MB): 280.66\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "'''\n",
    "device = \"cuda\"\n",
    "inp = torch.rand(1, 3, 224, 224).to(device)\n",
    "resnet = models.resnet50().to(device)\n",
    "\n",
    "summary(resnet, inp) # 250 = batch\n",
    "'''\n",
    "\n",
    "resnet = models.resnet50()\n",
    "summary(resnet, (1,3,224,224))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc8610-07ed-4b32-8354-76cc4e681a69",
   "metadata": {},
   "source": [
    "마지막의 Estimated Total Size를 참조하면 실제 학습할 때 사용할 GPU 양을 계산할 수 있습니다.   \n",
    "이를 통해 현재 GPU 자원을 기준으로 적당한 batch 크기를 계산할 수 있습니다.    \n",
    "Total mult-adds (G): 4.09 이게 flops다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f14ed5-442f-48ea-bc11-2c7f0bd80bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c70f1-c3d4-4912-80b2-c14e6677f109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf44311-5431-441e-9a65-d650b57b67d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45830f-9765-4a41-b5bc-c02765f12eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab030519-b196-4ef7-9441-edc6889c2852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da8cd75d-ab2f-4d09-8f22-cfdb792ec498",
   "metadata": {},
   "source": [
    "-----\n",
    "# FLOPs compute\n",
    "pthflops라는 라이브러리 다운로드받고 flops계산했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7333b4-7a2c-4303-b4a9-ab7dd10cd014",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pthflops\n",
      "  Downloading pthflops-0.4.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pthflops) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->pthflops) (4.1.1)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->pthflops) (0.8)\n",
      "Installing collected packages: pthflops\n",
      "Successfully installed pthflops-0.4.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pthflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86e13935-1246-4b3a-bea6-dcd1399f9f48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation               OPS          \n",
      "----------------------  -----------  \n",
      "conv1                   7552892928   \n",
      "bn1                     102760448    \n",
      "relu                    102760448    \n",
      "maxpool                 102760448    \n",
      "layer1_0_conv1          7398752256   \n",
      "layer1_0_bn1            25690112     \n",
      "layer1_0_relu           25690112     \n",
      "layer1_0_conv2          7398752256   \n",
      "layer1_0_bn2            25690112     \n",
      "add                     25690112     \n",
      "layer1_0_relu_1         25690112     \n",
      "layer1_1_conv1          7398752256   \n",
      "layer1_1_bn1            25690112     \n",
      "layer1_1_relu           25690112     \n",
      "layer1_1_conv2          7398752256   \n",
      "layer1_1_bn2            25690112     \n",
      "add_1                   25690112     \n",
      "layer1_1_relu_1         25690112     \n",
      "layer2_0_conv1          3699376128   \n",
      "layer2_0_bn1            12845056     \n",
      "layer2_0_relu           12845056     \n",
      "layer2_0_conv2          7398752256   \n",
      "layer2_0_bn2            12845056     \n",
      "layer2_0_downsample_0   411041792    \n",
      "layer2_0_downsample_1   12845056     \n",
      "add_2                   12845056     \n",
      "layer2_0_relu_1         12845056     \n",
      "layer2_1_conv1          7398752256   \n",
      "layer2_1_bn1            12845056     \n",
      "layer2_1_relu           12845056     \n",
      "layer2_1_conv2          7398752256   \n",
      "layer2_1_bn2            12845056     \n",
      "add_3                   12845056     \n",
      "layer2_1_relu_1         12845056     \n",
      "layer3_0_conv1          3699376128   \n",
      "layer3_0_bn1            6422528      \n",
      "layer3_0_relu           6422528      \n",
      "layer3_0_conv2          7398752256   \n",
      "layer3_0_bn2            6422528      \n",
      "layer3_0_downsample_0   411041792    \n",
      "layer3_0_downsample_1   6422528      \n",
      "add_4                   6422528      \n",
      "layer3_0_relu_1         6422528      \n",
      "layer3_1_conv1          7398752256   \n",
      "layer3_1_bn1            6422528      \n",
      "layer3_1_relu           6422528      \n",
      "layer3_1_conv2          7398752256   \n",
      "layer3_1_bn2            6422528      \n",
      "add_5                   6422528      \n",
      "layer3_1_relu_1         6422528      \n",
      "layer4_0_conv1          3699376128   \n",
      "layer4_0_bn1            3211264      \n",
      "layer4_0_relu           3211264      \n",
      "layer4_0_conv2          7398752256   \n",
      "layer4_0_bn2            3211264      \n",
      "layer4_0_downsample_0   411041792    \n",
      "layer4_0_downsample_1   3211264      \n",
      "add_6                   3211264      \n",
      "layer4_0_relu_1         3211264      \n",
      "layer4_1_conv1          7398752256   \n",
      "layer4_1_bn1            3211264      \n",
      "layer4_1_relu           3211264      \n",
      "layer4_1_conv2          7398752256   \n",
      "layer4_1_bn2            3211264      \n",
      "add_7                   3211264      \n",
      "layer4_1_relu_1         3211264      \n",
      "avgpool                 1605632      \n",
      "fc                      32769000     \n",
      "---------------------   ----------   \n",
      "Input size: (64, 3, 224, 224)\n",
      "116,914,750,440 FLOPs or approx. 116.91 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(116914750440,\n",
       " [['conv1', 7552892928],\n",
       "  ['bn1', 102760448],\n",
       "  ['relu', 102760448],\n",
       "  ['maxpool', 102760448],\n",
       "  ['layer1_0_conv1', 7398752256],\n",
       "  ['layer1_0_bn1', 25690112],\n",
       "  ['layer1_0_relu', 25690112],\n",
       "  ['layer1_0_conv2', 7398752256],\n",
       "  ['layer1_0_bn2', 25690112],\n",
       "  ['add', 25690112],\n",
       "  ['layer1_0_relu_1', 25690112],\n",
       "  ['layer1_1_conv1', 7398752256],\n",
       "  ['layer1_1_bn1', 25690112],\n",
       "  ['layer1_1_relu', 25690112],\n",
       "  ['layer1_1_conv2', 7398752256],\n",
       "  ['layer1_1_bn2', 25690112],\n",
       "  ['add_1', 25690112],\n",
       "  ['layer1_1_relu_1', 25690112],\n",
       "  ['layer2_0_conv1', 3699376128],\n",
       "  ['layer2_0_bn1', 12845056],\n",
       "  ['layer2_0_relu', 12845056],\n",
       "  ['layer2_0_conv2', 7398752256],\n",
       "  ['layer2_0_bn2', 12845056],\n",
       "  ['layer2_0_downsample_0', 411041792],\n",
       "  ['layer2_0_downsample_1', 12845056],\n",
       "  ['add_2', 12845056],\n",
       "  ['layer2_0_relu_1', 12845056],\n",
       "  ['layer2_1_conv1', 7398752256],\n",
       "  ['layer2_1_bn1', 12845056],\n",
       "  ['layer2_1_relu', 12845056],\n",
       "  ['layer2_1_conv2', 7398752256],\n",
       "  ['layer2_1_bn2', 12845056],\n",
       "  ['add_3', 12845056],\n",
       "  ['layer2_1_relu_1', 12845056],\n",
       "  ['layer3_0_conv1', 3699376128],\n",
       "  ['layer3_0_bn1', 6422528],\n",
       "  ['layer3_0_relu', 6422528],\n",
       "  ['layer3_0_conv2', 7398752256],\n",
       "  ['layer3_0_bn2', 6422528],\n",
       "  ['layer3_0_downsample_0', 411041792],\n",
       "  ['layer3_0_downsample_1', 6422528],\n",
       "  ['add_4', 6422528],\n",
       "  ['layer3_0_relu_1', 6422528],\n",
       "  ['layer3_1_conv1', 7398752256],\n",
       "  ['layer3_1_bn1', 6422528],\n",
       "  ['layer3_1_relu', 6422528],\n",
       "  ['layer3_1_conv2', 7398752256],\n",
       "  ['layer3_1_bn2', 6422528],\n",
       "  ['add_5', 6422528],\n",
       "  ['layer3_1_relu_1', 6422528],\n",
       "  ['layer4_0_conv1', 3699376128],\n",
       "  ['layer4_0_bn1', 3211264],\n",
       "  ['layer4_0_relu', 3211264],\n",
       "  ['layer4_0_conv2', 7398752256],\n",
       "  ['layer4_0_bn2', 3211264],\n",
       "  ['layer4_0_downsample_0', 411041792],\n",
       "  ['layer4_0_downsample_1', 3211264],\n",
       "  ['add_6', 3211264],\n",
       "  ['layer4_0_relu_1', 3211264],\n",
       "  ['layer4_1_conv1', 7398752256],\n",
       "  ['layer4_1_bn1', 3211264],\n",
       "  ['layer4_1_relu', 3211264],\n",
       "  ['layer4_1_conv2', 7398752256],\n",
       "  ['layer4_1_bn2', 3211264],\n",
       "  ['add_7', 3211264],\n",
       "  ['layer4_1_relu_1', 3211264],\n",
       "  ['avgpool', 1605632],\n",
       "  ['fc', 32769000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from pthflops import count_ops\n",
    "\n",
    "# Create a network and a corresponding input\n",
    "device = 'cuda:0'\n",
    "model = resnet18().to(device)\n",
    "inp = torch.rand(64,3,224,224).to(device)\n",
    "\n",
    "# Count the number of FLOPs\n",
    "count_ops(model, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c598f2f7-ecff-4d00-9f7d-ce7ac8c60911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation               OPS         \n",
      "----------------------  ----------  \n",
      "conv1                   118013952   \n",
      "bn1                     1605632     \n",
      "relu                    1605632     \n",
      "maxpool                 1605632     \n",
      "layer1_0_conv1          12845056    \n",
      "layer1_0_bn1            401408      \n",
      "layer1_0_relu           401408      \n",
      "layer1_0_conv2          115605504   \n",
      "layer1_0_bn2            401408      \n",
      "layer1_0_relu_1         401408      \n",
      "layer1_0_conv3          51380224    \n",
      "layer1_0_bn3            1605632     \n",
      "layer1_0_downsample_0   51380224    \n",
      "layer1_0_downsample_1   1605632     \n",
      "add                     1605632     \n",
      "layer1_0_relu_2         1605632     \n",
      "layer1_1_conv1          51380224    \n",
      "layer1_1_bn1            401408      \n",
      "layer1_1_relu           401408      \n",
      "layer1_1_conv2          115605504   \n",
      "layer1_1_bn2            401408      \n",
      "layer1_1_relu_1         401408      \n",
      "layer1_1_conv3          51380224    \n",
      "layer1_1_bn3            1605632     \n",
      "add_1                   1605632     \n",
      "layer1_1_relu_2         1605632     \n",
      "layer1_2_conv1          51380224    \n",
      "layer1_2_bn1            401408      \n",
      "layer1_2_relu           401408      \n",
      "layer1_2_conv2          115605504   \n",
      "layer1_2_bn2            401408      \n",
      "layer1_2_relu_1         401408      \n",
      "layer1_2_conv3          51380224    \n",
      "layer1_2_bn3            1605632     \n",
      "add_2                   1605632     \n",
      "layer1_2_relu_2         1605632     \n",
      "layer2_0_conv1          102760448   \n",
      "layer2_0_bn1            802816      \n",
      "layer2_0_relu           802816      \n",
      "layer2_0_conv2          115605504   \n",
      "layer2_0_bn2            200704      \n",
      "layer2_0_relu_1         200704      \n",
      "layer2_0_conv3          51380224    \n",
      "layer2_0_bn3            802816      \n",
      "layer2_0_downsample_0   102760448   \n",
      "layer2_0_downsample_1   802816      \n",
      "add_3                   802816      \n",
      "layer2_0_relu_2         802816      \n",
      "layer2_1_conv1          51380224    \n",
      "layer2_1_bn1            200704      \n",
      "layer2_1_relu           200704      \n",
      "layer2_1_conv2          115605504   \n",
      "layer2_1_bn2            200704      \n",
      "layer2_1_relu_1         200704      \n",
      "layer2_1_conv3          51380224    \n",
      "layer2_1_bn3            802816      \n",
      "add_4                   802816      \n",
      "layer2_1_relu_2         802816      \n",
      "layer2_2_conv1          51380224    \n",
      "layer2_2_bn1            200704      \n",
      "layer2_2_relu           200704      \n",
      "layer2_2_conv2          115605504   \n",
      "layer2_2_bn2            200704      \n",
      "layer2_2_relu_1         200704      \n",
      "layer2_2_conv3          51380224    \n",
      "layer2_2_bn3            802816      \n",
      "add_5                   802816      \n",
      "layer2_2_relu_2         802816      \n",
      "layer2_3_conv1          51380224    \n",
      "layer2_3_bn1            200704      \n",
      "layer2_3_relu           200704      \n",
      "layer2_3_conv2          115605504   \n",
      "layer2_3_bn2            200704      \n",
      "layer2_3_relu_1         200704      \n",
      "layer2_3_conv3          51380224    \n",
      "layer2_3_bn3            802816      \n",
      "add_6                   802816      \n",
      "layer2_3_relu_2         802816      \n",
      "layer3_0_conv1          102760448   \n",
      "layer3_0_bn1            401408      \n",
      "layer3_0_relu           401408      \n",
      "layer3_0_conv2          115605504   \n",
      "layer3_0_bn2            100352      \n",
      "layer3_0_relu_1         100352      \n",
      "layer3_0_conv3          51380224    \n",
      "layer3_0_bn3            401408      \n",
      "layer3_0_downsample_0   102760448   \n",
      "layer3_0_downsample_1   401408      \n",
      "add_7                   401408      \n",
      "layer3_0_relu_2         401408      \n",
      "layer3_1_conv1          51380224    \n",
      "layer3_1_bn1            100352      \n",
      "layer3_1_relu           100352      \n",
      "layer3_1_conv2          115605504   \n",
      "layer3_1_bn2            100352      \n",
      "layer3_1_relu_1         100352      \n",
      "layer3_1_conv3          51380224    \n",
      "layer3_1_bn3            401408      \n",
      "add_8                   401408      \n",
      "layer3_1_relu_2         401408      \n",
      "layer3_2_conv1          51380224    \n",
      "layer3_2_bn1            100352      \n",
      "layer3_2_relu           100352      \n",
      "layer3_2_conv2          115605504   \n",
      "layer3_2_bn2            100352      \n",
      "layer3_2_relu_1         100352      \n",
      "layer3_2_conv3          51380224    \n",
      "layer3_2_bn3            401408      \n",
      "add_9                   401408      \n",
      "layer3_2_relu_2         401408      \n",
      "layer3_3_conv1          51380224    \n",
      "layer3_3_bn1            100352      \n",
      "layer3_3_relu           100352      \n",
      "layer3_3_conv2          115605504   \n",
      "layer3_3_bn2            100352      \n",
      "layer3_3_relu_1         100352      \n",
      "layer3_3_conv3          51380224    \n",
      "layer3_3_bn3            401408      \n",
      "add_10                  401408      \n",
      "layer3_3_relu_2         401408      \n",
      "layer3_4_conv1          51380224    \n",
      "layer3_4_bn1            100352      \n",
      "layer3_4_relu           100352      \n",
      "layer3_4_conv2          115605504   \n",
      "layer3_4_bn2            100352      \n",
      "layer3_4_relu_1         100352      \n",
      "layer3_4_conv3          51380224    \n",
      "layer3_4_bn3            401408      \n",
      "add_11                  401408      \n",
      "layer3_4_relu_2         401408      \n",
      "layer3_5_conv1          51380224    \n",
      "layer3_5_bn1            100352      \n",
      "layer3_5_relu           100352      \n",
      "layer3_5_conv2          115605504   \n",
      "layer3_5_bn2            100352      \n",
      "layer3_5_relu_1         100352      \n",
      "layer3_5_conv3          51380224    \n",
      "layer3_5_bn3            401408      \n",
      "add_12                  401408      \n",
      "layer3_5_relu_2         401408      \n",
      "layer4_0_conv1          102760448   \n",
      "layer4_0_bn1            200704      \n",
      "layer4_0_relu           200704      \n",
      "layer4_0_conv2          115605504   \n",
      "layer4_0_bn2            50176       \n",
      "layer4_0_relu_1         50176       \n",
      "layer4_0_conv3          51380224    \n",
      "layer4_0_bn3            200704      \n",
      "layer4_0_downsample_0   102760448   \n",
      "layer4_0_downsample_1   200704      \n",
      "add_13                  200704      \n",
      "layer4_0_relu_2         200704      \n",
      "layer4_1_conv1          51380224    \n",
      "layer4_1_bn1            50176       \n",
      "layer4_1_relu           50176       \n",
      "layer4_1_conv2          115605504   \n",
      "layer4_1_bn2            50176       \n",
      "layer4_1_relu_1         50176       \n",
      "layer4_1_conv3          51380224    \n",
      "layer4_1_bn3            200704      \n",
      "add_14                  200704      \n",
      "layer4_1_relu_2         200704      \n",
      "layer4_2_conv1          51380224    \n",
      "layer4_2_bn1            50176       \n",
      "layer4_2_relu           50176       \n",
      "layer4_2_conv2          115605504   \n",
      "layer4_2_bn2            50176       \n",
      "layer4_2_relu_1         50176       \n",
      "layer4_2_conv3          51380224    \n",
      "layer4_2_bn3            200704      \n",
      "add_15                  200704      \n",
      "layer4_2_relu_2         200704      \n",
      "avgpool                 100352      \n",
      "fc                      2049000     \n",
      "---------------------   ---------   \n",
      "Input size: (1, 3, 224, 224)\n",
      "4,143,375,336 FLOPs or approx. 4.14 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncustom resnet50 = 3.89 GFLOPs\\ntorchvision resnet50 = 4.14 GFLOPs\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50, mobilenet_v2\n",
    "#from resnet import ResNet50\n",
    "\n",
    "from pthflops import count_ops\n",
    "\n",
    "# Create a network and a corresponding input\n",
    "device = 'cuda:1'\n",
    "model = resnet50().to(device) \n",
    "#MobileNetV2 = mobilenet_v2().to(device)\n",
    "inp = torch.rand(1,3,224,224).to(device)\n",
    "\n",
    "# Count the number of FLOPs\n",
    "count_ops(model, inp)\n",
    "#count_ops(MobileNetV2, inp)\n",
    "\n",
    "'''\n",
    "custom resnet50 = 3.89 GFLOPs\n",
    "torchvision resnet50 = 4.14 GFLOPs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ca37a-eec9-4bf3-8796-cbae525dd7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
