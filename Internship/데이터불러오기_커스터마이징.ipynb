{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=8PnxJ3s3Cwo&list=PLHOsBEAyYj3xf4i20sCA5o8MgVW5sIiHD&index=11   \n",
    "위 유튜브 기준 + Docs 참조한 내용 정리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as tr\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchvision.transforms란?   \n",
    "데이터를 불러오면서 바로 전처리할 수 있게 해주는 라이브러리  \n",
    "   \n",
    "#### DataLoader란?   \n",
    "데이터를 배치사이즈 형태로 만들어서, 실제로 학습할 때 이용할 수 있도록 만드는 라이브러리   \n",
    "**Docs: Dataset을 샘플에 쉽게 접근할 수 있게 순회 가능한 객체(iterable)로 감쌈.**\n",
    "#### Dataset란?\n",
    "튜닝할 때 사용할 껀데, 차후 말씀드림.   \n",
    "**Docs: 샘플과 정답(label)을 저장**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 파이토치 제공 데이터 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.Compose란?\n",
    "안에있는 내용 1열로 순차적 수행.   \n",
    "Compose안에는 PIL이미지만 들어갈 수 있다.(https://velog.io/@olxtar/Torchvision-PIL-torch.Tensor-PIL-Image <= PIL,numpy,tensor 이미지 정리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd43adc71d7943e5ac942870f08921fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transf = tr.Compose([tr.Resize(8), tr.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transf)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].size()\n",
    "#torch.Size([3, 8, 8]) = Channel, Hight, Width\n",
    "#일반적인 이미지 = 8,8,3 = H,W,C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fe4b5aeb4de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "trainset[0][1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader란?\n",
    "Dataset을 batch 기반의 딥러닝 모델 학습을 위해 배치형태로 만들어서 우리가 실제로 학습할 때 이용할 수 있게 형태를 만들어 주는 기능\n",
    "\n",
    "#### num_workers란?\n",
    "학습 도중 CPU의 작업을 몇 개의 코어를 사용해서 진행할 지에 대한 설정 파라미터.    \n",
    "https://jybaek.tistory.com/799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=50, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=50, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader) # CIFAR가 50000개인데, batch를 50으로 했으니까 50짜리 batch 1000개 나오는거다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainloader안에 실제 값을 보고싶다면?\n",
    "##### iter, next\n",
    "한 묶음씩 불러오겠다는 뜻."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 8, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size() #[50, 3, 8, 8] = [batch_size, channel, hight, width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 같은 클래스 별 폴더 이미지 데이터 이용\n",
    "우리가 정리를 잘해놔서 폴더안에 클래스별로 나눠논 상황이라면?   \n",
    "*'./class/tiger'*   \n",
    "*'./class/lion'*     \n",
    "이런식으로   \n",
    "   \n",
    "   \n",
    "#### torchvision.datasets.ImageFolder <-이걸써라\n",
    "*'./class'* 안에 있는 이미지들을 알아서 Search해주고,   \n",
    "각각의 폴더에 대해서 labeling을 알아서 해줌!!! 엄청 좋죠?   \n",
    "추가적으로 *transform=transf*을 통해서 전처리를 해줄 수 있다는것.   \n",
    "정리하자면,   \n",
    "*trainset = torchvision.datasets.ImageFolder(root='./class', transform=transf)*   \n",
    "이 한줄로 **데이터 불러오기 + 라벨링 + 전처리 ** 쌉possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c629c65f6f3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtransf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './class'"
     ]
    }
   ],
   "source": [
    "transf = tr.Compose([tr.Resize(16), tr.ToTensor()])\n",
    "trainset = torchvision.datasets.ImageFolder(root='./class', transform=transf)\n",
    "trainloader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=2)\n",
    "print(len(trainloader))\n",
    "#파일없어서 에러남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. (개인데이터)직접 데이터 불러오기 (2가지 방법)\n",
    "### 3-1 첫 번째 방법은 딥러닝호형이 선호하는 방법   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 32, 32, 3) (20, 1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "넘파이 형태로 들어왔다고 가정,\n",
    "32x32, 3Ch짜리 이미지가 20개 있다고 가정\n",
    "그에 따른 레이블 20개 있다고 가정\n",
    "'''\n",
    "\n",
    "train_images = np.random.randint(256, size=(20, 32, 32,3))\n",
    "train_labels = np.random.randint(2, size=(20, 1))\n",
    "\n",
    "print(train_images.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수동적인 데이터를 불러오려면, 아래 ***코드 양식***을 기억해라!!!!!@!#@!#!@#\n",
    "# init, getitem, len함수는 정해져있고 세부 내용은 변경하면 된다고 생각해라.\n",
    "\n",
    "class TensorData(Dataset):\n",
    "    \n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.FloatTensor(x_data)\n",
    "        self.x_data = self.x_data.permute(0, 3, 1, 2) # permute함수를 이용해서 순서를 바꿔주도록 한 것임.\n",
    "                                                      # 이미지 개수, 채널 수, 이미지 높이, 너비\n",
    "        self.y_data = torch.LongTensor(y_data)\n",
    "        self.len = self.y_data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorData(train_images, train_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 32, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [번외] 호형이 torchvision.datasets.ImageFolder를 쓰지 않는 개인적인 이유\n",
    "- 폴더 정리 및 변경을 하지 못하는 경우\n",
    "    - 다른 작업과 공용으로 사용\n",
    "    - 폴더가 아닌 SQL 같은 곳에서 넘어오는 경우   \n",
    "\n",
    "\n",
    "- 파이토치에서 제공하는 transform 종류가 제한적\n",
    "    - OpenCV가 훨씬 더 많음\n",
    "    - 직접 이미지 전처리에 대한 함수를 만들어야 되는 경우도 있음.\n",
    "    - 이러한 디테일한 이유 때문에 파이토치에서 제공하는 전처리 작업은 사용 안함.\n",
    "    \n",
    "***그래서, 호형은 데이터 전처리 \"모듈\"을 따로 만들어놓고 사용한다네***   \n",
    "ex.   \n",
    "import preprocessing   \n",
    "\n",
    "train_images, train_labels = preprocessing(train_images, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치로 무조건 전처리하기(고급ver)\n",
    "https://www.youtube.com/watch?v=8PnxJ3s3Cwo&list=PLHOsBEAyYj3xf4i20sCA5o8MgVW5sIiHD&index=11   \n",
    "16분정도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "개인 데이터에다가 transform 이용하기.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x_data, y_data, transform=None):\n",
    "        \n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.transform = transform\n",
    "        self.len = len(y_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, labels = sample\n",
    "        inputs = torch.FloatTensor(inputs)\n",
    "        inputs = inputs.permute(2, 0, 1)\n",
    "        return inputs, torch.LongTensor(labels)\n",
    "\n",
    "class LinearTensor:\n",
    "    \n",
    "    def __init__(self, slope=1, bias=0):\n",
    "        self.slope = slope\n",
    "        self.bias = bias\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        inputs, labels = sample\n",
    "        inputs = self.slope*inputs + self.bias\n",
    "        return inputs, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = tr.Compose([ToTensor(), LinearTensor(2, 5)])\n",
    "ds1 = MyDataset(train_images, train_labels, transform=trans)\n",
    "train_loader1 = DataLoader(ds1, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "first_data = ds1[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter1 = iter(train_loader1)\n",
    "images1, labels1 = dataiter1.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 75., 305., 491.,  ..., 415., 311., 115.],\n",
       "          [227., 235., 351.,  ..., 401., 161.,   5.],\n",
       "          [111., 441., 451.,  ..., 449.,   9.,  99.],\n",
       "          ...,\n",
       "          [445., 123.,  23.,  ..., 333., 403., 271.],\n",
       "          [273., 213., 147.,  ..., 305., 169., 367.],\n",
       "          [ 43., 195., 491.,  ..., 279.,  95.,  79.]],\n",
       "\n",
       "         [[391.,  31., 445.,  ..., 269., 185., 195.],\n",
       "          [497., 477., 411.,  ..., 353., 297., 103.],\n",
       "          [321., 289., 373.,  ..., 441., 361.,  51.],\n",
       "          ...,\n",
       "          [321., 211.,  73.,  ..., 355., 187., 263.],\n",
       "          [465., 217.,  79.,  ..., 273., 229., 233.],\n",
       "          [233., 377., 455.,  ..., 265., 133.,  63.]],\n",
       "\n",
       "         [[ 31., 337., 515.,  ..., 197., 241., 265.],\n",
       "          [155., 175., 445.,  ..., 243.,  39., 349.],\n",
       "          [201., 511., 235.,  ..., 295., 345., 391.],\n",
       "          ...,\n",
       "          [131., 471., 265.,  ..., 203., 127., 131.],\n",
       "          [447., 317., 441.,  ...,  49.,  11., 461.],\n",
       "          [ 57., 139.,  45.,  ...,  65., 331., 447.]]],\n",
       "\n",
       "\n",
       "        [[[161., 403., 275.,  ..., 203., 143.,  53.],\n",
       "          [ 29.,  55., 151.,  ..., 449., 169., 413.],\n",
       "          [ 27., 373., 243.,  ..., 493., 253., 441.],\n",
       "          ...,\n",
       "          [249., 101., 467.,  ...,  73., 245., 441.],\n",
       "          [275., 211., 399.,  ..., 327., 309., 327.],\n",
       "          [239., 387., 379.,  ...,  81., 477., 423.]],\n",
       "\n",
       "         [[195., 433., 185.,  ..., 255., 409., 259.],\n",
       "          [271.,  71., 499.,  ...,  45., 355.,  93.],\n",
       "          [265.,  27., 111.,  ..., 261., 339., 413.],\n",
       "          ...,\n",
       "          [319., 371., 369.,  ..., 475., 257., 255.],\n",
       "          [ 25., 109., 247.,  ..., 347., 295.,  43.],\n",
       "          [ 71., 131., 483.,  ...,  75., 427., 105.]],\n",
       "\n",
       "         [[397., 175., 107.,  ..., 423., 321.,   9.],\n",
       "          [303., 463., 391.,  ..., 139.,  25., 489.],\n",
       "          [221., 437., 485.,  ..., 351., 471., 329.],\n",
       "          ...,\n",
       "          [431., 481.,   9.,  ..., 209., 275., 275.],\n",
       "          [419., 353., 219.,  ..., 383., 425., 237.],\n",
       "          [  5., 359., 375.,  ..., 201., 401., 399.]]],\n",
       "\n",
       "\n",
       "        [[[489.,  43., 379.,  ...,  93., 361., 379.],\n",
       "          [231., 367., 371.,  ..., 261.,  25., 343.],\n",
       "          [ 19., 467., 249.,  ..., 183., 273., 263.],\n",
       "          ...,\n",
       "          [371., 327.,  79.,  ..., 459., 341., 419.],\n",
       "          [ 59.,  51.,  95.,  ...,  65., 345., 333.],\n",
       "          [183.,  49., 377.,  ..., 453.,  11., 167.]],\n",
       "\n",
       "         [[291., 285., 211.,  ...,  61., 133.,  35.],\n",
       "          [457.,  49., 427.,  ...,  73.,  81., 459.],\n",
       "          [235., 359., 247.,  ...,  25., 255., 141.],\n",
       "          ...,\n",
       "          [389., 459., 393.,  ..., 261., 457., 225.],\n",
       "          [291., 233., 237.,  ..., 189., 447., 125.],\n",
       "          [279., 329., 225.,  ..., 331., 391., 217.]],\n",
       "\n",
       "         [[373.,  11.,  79.,  ..., 481., 361., 451.],\n",
       "          [297.,  29., 249.,  ..., 261., 275.,  81.],\n",
       "          [451., 215., 277.,  ..., 389., 225., 213.],\n",
       "          ...,\n",
       "          [353., 205., 493.,  ..., 227., 169., 215.],\n",
       "          [115., 205., 417.,  ..., 189., 301., 501.],\n",
       "          [215., 159.,   9.,  ..., 257.,  25., 161.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[123., 113., 263.,  ..., 503., 169., 357.],\n",
       "          [389., 481., 341.,  ..., 455., 277., 287.],\n",
       "          [453., 187., 341.,  ..., 193.,  57., 193.],\n",
       "          ...,\n",
       "          [103., 217., 385.,  ..., 435., 239., 429.],\n",
       "          [237., 255., 291.,  ..., 415., 351.,  97.],\n",
       "          [185., 415.,  49.,  ..., 295.,  17., 445.]],\n",
       "\n",
       "         [[109., 323., 127.,  ..., 125.,  89., 451.],\n",
       "          [287., 459., 117.,  ..., 309.,  77., 407.],\n",
       "          [187., 413., 407.,  ..., 487., 121., 187.],\n",
       "          ...,\n",
       "          [483., 419.,  75.,  ..., 269., 421.,  59.],\n",
       "          [ 11., 149.,  79.,  ...,  83., 499., 207.],\n",
       "          [503., 173., 411.,  ..., 141.,  93., 377.]],\n",
       "\n",
       "         [[477.,  29., 273.,  ..., 343., 355., 399.],\n",
       "          [299., 425., 213.,  ..., 331., 173., 347.],\n",
       "          [243., 307.,  19.,  ...,  65., 167., 171.],\n",
       "          ...,\n",
       "          [ 63.,  75., 267.,  ..., 337.,  93.,  93.],\n",
       "          [181.,  87., 227.,  ..., 149., 395., 273.],\n",
       "          [217., 477., 283.,  ..., 297., 195., 295.]]],\n",
       "\n",
       "\n",
       "        [[[ 91.,  55., 329.,  ...,  53., 121., 495.],\n",
       "          [139., 229.,  61.,  ...,  99., 285., 449.],\n",
       "          [235., 339.,  73.,  ..., 249., 463.,  15.],\n",
       "          ...,\n",
       "          [259., 447., 417.,  ..., 353., 507., 207.],\n",
       "          [325., 425., 169.,  ..., 267., 241., 341.],\n",
       "          [441., 465., 375.,  ...,  21., 395., 341.]],\n",
       "\n",
       "         [[351.,  75.,  95.,  ...,  71., 495., 485.],\n",
       "          [197.,  69.,  61.,  ...,  25., 339., 259.],\n",
       "          [455., 311., 141.,  ..., 481.,  75., 239.],\n",
       "          ...,\n",
       "          [119., 357., 495.,  ...,  67., 361., 329.],\n",
       "          [223., 289., 361.,  ..., 197., 513., 347.],\n",
       "          [169., 443.,  77.,  ..., 323., 231., 437.]],\n",
       "\n",
       "         [[343., 163., 157.,  ..., 143., 403., 245.],\n",
       "          [135., 287., 427.,  ..., 399., 109., 439.],\n",
       "          [473., 243., 105.,  ...,  29., 361., 477.],\n",
       "          ...,\n",
       "          [247., 375., 365.,  ..., 399., 327., 371.],\n",
       "          [127., 111., 477.,  ..., 473., 351.,  51.],\n",
       "          [ 31.,  93., 107.,  ..., 167.,  69., 275.]]],\n",
       "\n",
       "\n",
       "        [[[457.,   9., 155.,  ..., 323., 261., 495.],\n",
       "          [351., 117., 303.,  ..., 391.,  45., 385.],\n",
       "          [217., 201., 323.,  ..., 111., 209., 509.],\n",
       "          ...,\n",
       "          [435., 187.,  91.,  ..., 493., 199., 317.],\n",
       "          [153., 369., 341.,  ..., 155., 313., 165.],\n",
       "          [207., 501.,  81.,  ...,  23., 469., 285.]],\n",
       "\n",
       "         [[281., 387., 245.,  ..., 117., 393., 461.],\n",
       "          [495.,  41., 145.,  ...,  19., 109., 437.],\n",
       "          [231.,  23., 509.,  ..., 113., 515., 183.],\n",
       "          ...,\n",
       "          [409., 289., 499.,  ..., 481., 131., 321.],\n",
       "          [381., 339.,  43.,  ..., 513.,  61., 265.],\n",
       "          [143.,  87., 477.,  ..., 145., 363., 237.]],\n",
       "\n",
       "         [[175., 259., 503.,  ..., 191., 165., 371.],\n",
       "          [297., 381., 283.,  ..., 343., 269., 297.],\n",
       "          [357., 425.,  81.,  ..., 119., 391., 225.],\n",
       "          ...,\n",
       "          [215., 433., 331.,  ..., 353., 101.,  13.],\n",
       "          [503., 443., 247.,  ..., 293., 267., 149.],\n",
       "          [459., 405., 473.,  ..., 291., 507., 169.]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개인 데이터 + torchvision.transforms(고급)\n",
    "tr.compose이용해서 하고싶을경우(class재정의하는게 너무 힘들어요!!~~일경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x_data, y_data, transform=None):\n",
    "        \n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.transform = transform\n",
    "        self.len = len(y_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class MyTransform:\n",
    "    \n",
    "    def __call__(self, sample):        \n",
    "        inputs, labels = sample\n",
    "        inputs = torch.FloatTensor(inputs)\n",
    "        inputs = inputs.permute(2, 0, 1)\n",
    "        labels = torch.FloatTensor(labels)\n",
    "        \n",
    "        transf = tr.Compose([tr.ToPILImage(), tr.Resize(128),tr.ToTensor(), tr.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        final_output = transf(inputs)\n",
    "        \n",
    "        return final_output, labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = MyDataset(train_images, train_labels, transform=MyTransform())\n",
    "train_loader2 = DataLoader(ds2, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "first_data = ds2[0]\n",
    "features, labels = first_data\n",
    "print(type(features), type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter2 = iter(train_loader2)\n",
    "images2 , labels2 = dataiter2.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3569,  0.3569,  0.2314,  ...,  0.1922,  0.2000,  0.2000],\n",
       "          [ 0.3569,  0.3569,  0.2314,  ...,  0.1922,  0.2000,  0.2000],\n",
       "          [ 0.2941,  0.2941,  0.1765,  ...,  0.0588,  0.0667,  0.0667],\n",
       "          ...,\n",
       "          [ 0.7569,  0.7569,  0.5843,  ...,  0.2157,  0.3333,  0.3333],\n",
       "          [ 0.8118,  0.8118,  0.6157,  ...,  0.2157,  0.3333,  0.3333],\n",
       "          [ 0.8118,  0.8118,  0.6157,  ...,  0.2157,  0.3333,  0.3333]],\n",
       "\n",
       "         [[ 0.8431,  0.8431,  0.7412,  ..., -0.2784, -0.3020, -0.3020],\n",
       "          [ 0.8431,  0.8431,  0.7412,  ..., -0.2784, -0.3020, -0.3020],\n",
       "          [ 0.8118,  0.8118,  0.7098,  ..., -0.2314, -0.2706, -0.2706],\n",
       "          ...,\n",
       "          [-0.2549, -0.2549, -0.2863,  ..., -0.2471, -0.3020, -0.3020],\n",
       "          [-0.1608, -0.1608, -0.2000,  ..., -0.3961, -0.4745, -0.4745],\n",
       "          [-0.1608, -0.1608, -0.2000,  ..., -0.3961, -0.4745, -0.4745]],\n",
       "\n",
       "         [[ 0.4667,  0.4667,  0.3490,  ...,  0.5059,  0.5137,  0.5137],\n",
       "          [ 0.4667,  0.4667,  0.3490,  ...,  0.5059,  0.5137,  0.5137],\n",
       "          [ 0.3490,  0.3490,  0.2549,  ...,  0.4667,  0.4667,  0.4667],\n",
       "          ...,\n",
       "          [-0.1294, -0.1294, -0.1843,  ..., -0.7725, -0.8510, -0.8510],\n",
       "          [-0.1922, -0.1922, -0.2471,  ..., -0.8510, -0.9451, -0.9451],\n",
       "          [-0.1922, -0.1922, -0.2471,  ..., -0.8510, -0.9451, -0.9451]]],\n",
       "\n",
       "\n",
       "        [[[-0.7647, -0.7647, -0.5451,  ..., -0.7961, -0.9137, -0.9137],\n",
       "          [-0.7647, -0.7647, -0.5451,  ..., -0.7961, -0.9137, -0.9137],\n",
       "          [-0.7098, -0.7098, -0.5059,  ..., -0.7333, -0.8588, -0.8588],\n",
       "          ...,\n",
       "          [ 0.2392,  0.2392,  0.1059,  ..., -0.1137, -0.0275, -0.0275],\n",
       "          [ 0.2157,  0.2157,  0.0745,  ..., -0.1765, -0.0902, -0.0902],\n",
       "          [ 0.2157,  0.2157,  0.0745,  ..., -0.1765, -0.0902, -0.0902]],\n",
       "\n",
       "         [[-0.0745, -0.0745, -0.1294,  ..., -0.7490, -0.7804, -0.7804],\n",
       "          [-0.0745, -0.0745, -0.1294,  ..., -0.7490, -0.7804, -0.7804],\n",
       "          [-0.1765, -0.1765, -0.2000,  ..., -0.7176, -0.7647, -0.7647],\n",
       "          ...,\n",
       "          [ 0.3490,  0.3490,  0.3804,  ...,  0.0431,  0.0824,  0.0824],\n",
       "          [ 0.4667,  0.4667,  0.4980,  ...,  0.0353,  0.0980,  0.0980],\n",
       "          [ 0.4667,  0.4667,  0.4980,  ...,  0.0353,  0.0980,  0.0980]],\n",
       "\n",
       "         [[ 0.3412,  0.3412,  0.3020,  ..., -0.3255, -0.4275, -0.4275],\n",
       "          [ 0.3412,  0.3412,  0.3020,  ..., -0.3255, -0.4275, -0.4275],\n",
       "          [ 0.2784,  0.2784,  0.2392,  ..., -0.3020, -0.3882, -0.3882],\n",
       "          ...,\n",
       "          [-0.7961, -0.7961, -0.7725,  ...,  0.2235,  0.3725,  0.3725],\n",
       "          [-0.7725, -0.7725, -0.7490,  ...,  0.2000,  0.3647,  0.3647],\n",
       "          [-0.7725, -0.7725, -0.7490,  ...,  0.2000,  0.3647,  0.3647]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2471,  0.2471,  0.2235,  ...,  0.3255,  0.3647,  0.3647],\n",
       "          [ 0.2471,  0.2471,  0.2235,  ...,  0.3255,  0.3647,  0.3647],\n",
       "          [ 0.1216,  0.1216,  0.1059,  ...,  0.2941,  0.3255,  0.3255],\n",
       "          ...,\n",
       "          [-0.2627, -0.2627, -0.2627,  ...,  0.2157,  0.1373,  0.1373],\n",
       "          [-0.3490, -0.3490, -0.3490,  ...,  0.1294,  0.0353,  0.0353],\n",
       "          [-0.3490, -0.3490, -0.3490,  ...,  0.1294,  0.0353,  0.0353]],\n",
       "\n",
       "         [[ 0.6000,  0.6000,  0.5451,  ...,  0.2000,  0.2078,  0.2078],\n",
       "          [ 0.6000,  0.6000,  0.5451,  ...,  0.2000,  0.2078,  0.2078],\n",
       "          [ 0.6000,  0.6000,  0.5373,  ...,  0.1843,  0.1843,  0.1843],\n",
       "          ...,\n",
       "          [-0.9373, -0.9373, -0.8902,  ..., -0.4353, -0.4039, -0.4039],\n",
       "          [-0.9765, -0.9765, -0.9451,  ..., -0.3961, -0.3255, -0.3255],\n",
       "          [-0.9765, -0.9765, -0.9451,  ..., -0.3961, -0.3255, -0.3255]],\n",
       "\n",
       "         [[ 0.4118,  0.4118,  0.3961,  ...,  0.1059,  0.0902,  0.0902],\n",
       "          [ 0.4118,  0.4118,  0.3961,  ...,  0.1059,  0.0902,  0.0902],\n",
       "          [ 0.3961,  0.3961,  0.3569,  ...,  0.0980,  0.0745,  0.0745],\n",
       "          ...,\n",
       "          [-0.1686, -0.1686, -0.2000,  ..., -0.0275,  0.0510,  0.0510],\n",
       "          [-0.1216, -0.1216, -0.1765,  ..., -0.1529, -0.0824, -0.0824],\n",
       "          [-0.1216, -0.1216, -0.1765,  ..., -0.1529, -0.0824, -0.0824]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.6706,  0.6706,  0.5373,  ...,  0.5843,  0.7176,  0.7176],\n",
       "          [ 0.6706,  0.6706,  0.5373,  ...,  0.5843,  0.7176,  0.7176],\n",
       "          [ 0.6706,  0.6706,  0.5451,  ...,  0.5922,  0.7098,  0.7098],\n",
       "          ...,\n",
       "          [ 0.7333,  0.7333,  0.6784,  ...,  0.8039,  0.7961,  0.7961],\n",
       "          [ 0.7647,  0.7647,  0.7176,  ...,  0.8510,  0.8353,  0.8353],\n",
       "          [ 0.7647,  0.7647,  0.7176,  ...,  0.8510,  0.8353,  0.8353]],\n",
       "\n",
       "         [[-0.3176, -0.3176, -0.3255,  ..., -0.7961, -0.7725, -0.7725],\n",
       "          [-0.3176, -0.3176, -0.3255,  ..., -0.7961, -0.7725, -0.7725],\n",
       "          [-0.3255, -0.3255, -0.3333,  ..., -0.7255, -0.7176, -0.7176],\n",
       "          ...,\n",
       "          [ 0.7647,  0.7647,  0.6549,  ..., -0.6235, -0.8431, -0.8431],\n",
       "          [ 0.8353,  0.8353,  0.7255,  ..., -0.6078, -0.8353, -0.8353],\n",
       "          [ 0.8353,  0.8353,  0.7255,  ..., -0.6078, -0.8353, -0.8353]],\n",
       "\n",
       "         [[-0.5843, -0.5843, -0.6000,  ..., -0.2471, -0.3961, -0.3961],\n",
       "          [-0.5843, -0.5843, -0.6000,  ..., -0.2471, -0.3961, -0.3961],\n",
       "          [-0.4824, -0.4824, -0.4902,  ..., -0.1686, -0.2706, -0.2706],\n",
       "          ...,\n",
       "          [ 0.6314,  0.6314,  0.6078,  ...,  0.2078,  0.2471,  0.2471],\n",
       "          [ 0.6549,  0.6549,  0.6471,  ...,  0.2941,  0.3490,  0.3490],\n",
       "          [ 0.6549,  0.6549,  0.6471,  ...,  0.2941,  0.3490,  0.3490]]],\n",
       "\n",
       "\n",
       "        [[[-0.3647, -0.3647, -0.4275,  ...,  0.3725,  0.3176,  0.3176],\n",
       "          [-0.3647, -0.3647, -0.4275,  ...,  0.3725,  0.3176,  0.3176],\n",
       "          [-0.3882, -0.3882, -0.4196,  ...,  0.3725,  0.3098,  0.3098],\n",
       "          ...,\n",
       "          [-0.2392, -0.2392, -0.2000,  ...,  0.5529,  0.6784,  0.6784],\n",
       "          [-0.3255, -0.3255, -0.2706,  ...,  0.5765,  0.7098,  0.7098],\n",
       "          [-0.3255, -0.3255, -0.2706,  ...,  0.5765,  0.7098,  0.7098]],\n",
       "\n",
       "         [[ 0.2941,  0.2941,  0.2941,  ..., -0.8510, -0.8667, -0.8667],\n",
       "          [ 0.2941,  0.2941,  0.2941,  ..., -0.8510, -0.8667, -0.8667],\n",
       "          [ 0.2471,  0.2471,  0.2471,  ..., -0.7020, -0.7098, -0.7098],\n",
       "          ...,\n",
       "          [-0.6392, -0.6392, -0.5529,  ..., -0.1765, -0.1765, -0.1765],\n",
       "          [-0.6706, -0.6706, -0.5765,  ..., -0.2392, -0.2471, -0.2471],\n",
       "          [-0.6706, -0.6706, -0.5765,  ..., -0.2392, -0.2471, -0.2471]],\n",
       "\n",
       "         [[-0.6706, -0.6706, -0.7020,  ...,  0.4353,  0.4039,  0.4039],\n",
       "          [-0.6706, -0.6706, -0.7020,  ...,  0.4353,  0.4039,  0.4039],\n",
       "          [-0.5529, -0.5529, -0.5922,  ...,  0.3569,  0.3255,  0.3255],\n",
       "          ...,\n",
       "          [ 0.7020,  0.7020,  0.7020,  ..., -0.4588, -0.4196, -0.4196],\n",
       "          [ 0.9059,  0.9059,  0.8824,  ..., -0.4118, -0.3647, -0.3647],\n",
       "          [ 0.9059,  0.9059,  0.8824,  ..., -0.4118, -0.3647, -0.3647]]],\n",
       "\n",
       "\n",
       "        [[[-0.0588, -0.0588, -0.1294,  ...,  0.5216,  0.7255,  0.7255],\n",
       "          [-0.0588, -0.0588, -0.1294,  ...,  0.5216,  0.7255,  0.7255],\n",
       "          [-0.0980, -0.0980, -0.1529,  ...,  0.4118,  0.5686,  0.5686],\n",
       "          ...,\n",
       "          [-0.1765, -0.1765, -0.0667,  ..., -0.3255, -0.4039, -0.4039],\n",
       "          [-0.0824, -0.0824,  0.0431,  ..., -0.4275, -0.5137, -0.5137],\n",
       "          [-0.0824, -0.0824,  0.0431,  ..., -0.4275, -0.5137, -0.5137]],\n",
       "\n",
       "         [[-0.9686, -0.9686, -0.9059,  ..., -0.7255, -0.8353, -0.8353],\n",
       "          [-0.9686, -0.9686, -0.9059,  ..., -0.7255, -0.8353, -0.8353],\n",
       "          [-0.9686, -0.9686, -0.9059,  ..., -0.7176, -0.8431, -0.8431],\n",
       "          ...,\n",
       "          [ 0.7333,  0.7333,  0.7569,  ..., -0.5059, -0.6627, -0.6627],\n",
       "          [ 0.9059,  0.9059,  0.9137,  ..., -0.5294, -0.6941, -0.6941],\n",
       "          [ 0.9059,  0.9059,  0.9137,  ..., -0.5294, -0.6941, -0.6941]],\n",
       "\n",
       "         [[ 0.8510,  0.8510,  0.6235,  ...,  0.3098,  0.4745,  0.4745],\n",
       "          [ 0.8510,  0.8510,  0.6235,  ...,  0.3098,  0.4745,  0.4745],\n",
       "          [ 0.7490,  0.7490,  0.5373,  ...,  0.3098,  0.4667,  0.4667],\n",
       "          ...,\n",
       "          [ 0.5294,  0.5294,  0.4353,  ...,  0.5922,  0.5608,  0.5608],\n",
       "          [ 0.6706,  0.6706,  0.5373,  ...,  0.6314,  0.5922,  0.5922],\n",
       "          [ 0.6706,  0.6706,  0.5373,  ...,  0.6314,  0.5922,  0.5922]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
